{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Collecting and Appending Data\n",
    "#### 11/28/2017 | Hunter Heaivilin | Sprint 2\n",
    "#### Description\n",
    "Exploring various tools to grab data (downloads, web scraping, APIs), clean it as needed, and scraped websites to add to the existing data.\n",
    "\n",
    "## Skill Backlog User Story\n",
    "As a researcher, I need to understand command line-based applications, basic scripting, databases, web resources, APIs, and data warehouses so that I can download data from websites, connect to and query databases, interface with APIs, and web scrape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible Projects\n",
    "\n",
    "- Go to website ([AGRICULTURAL DEDICATIONS](https://www.realpropertyhonolulu.com/dedications/agricultural-dedications/)\n",
    "- Grab pdf ([2017 Dedicated Agricultural Parcels list](https://www.realpropertyhonolulu.com/media/1465/ag.pdf)) \n",
    "- Parse pdf for relevant data \n",
    "- Compile data into some new format (e.g., csv), \n",
    "- Add a bit of the data onto a url as custom suffix\n",
    "- Go to url with custom suffix (e.g, http://qpublic9.qpublic.net/hi_honolulu_display.php?county=hi_honolulu&KEY=290170150000)\n",
    "- - Scrape data from url pages for more data that I want\n",
    "- add new data from url to existing dataset from pdf\n",
    "- join data with spatial dataset\n",
    "\n",
    "\n",
    "- Pull data from APIs ([Organic Integrity Database API](https://organic.ams.usda.gov/integrity/Developer/APIHelp.aspx) or one of [these](https://www.programmableweb.com/category/agriculture/api) ag related ones)\n",
    "- Geocode [2017 Dedicated Agricultural Parcels list](https://www.realpropertyhonolulu.com/media/1465/ag.pdf) from site address (or TMK if can find good data) to csv with lat long (via geocodio or similar) and map with Python using Shaply and Fiona.\n",
    "- Perform similar but using [Organic Integrity Database API](https://organic.ams.usda.gov/integrity/Developer/APIHelp.aspx) to map certified organic farms in the state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project Proposal\n",
    "--\n",
    "This is what I propose to do in this project and why I think it will be useful to me and my overall objective\n",
    "\n",
    "\n",
    "Go to a website ([AGRICULTURAL DEDICATIONS](https://www.realpropertyhonolulu.com/dedications/agricultural-dedications/) and download a pdf with tables in it([2017 Dedicated Agricultural Parcels list](https://www.realpropertyhonolulu.com/media/1465/ag.pdf)). Parse the pdf for relevant data and compile the data into a more useful format (e.g., csv). \n",
    "- Add a bit of the data onto a url as custom suffix\n",
    "- Go to url with custom suffix (e.g, http://qpublic9.qpublic.net/hi_honolulu_display.php?county=hi_honolulu&KEY=290170150000)\n",
    "- Parse url page for more data that I want\n",
    "- add new data from url to existing dataset from pdf\n",
    "- join data with spatial dataset\n",
    "\n",
    "\n",
    "\n",
    "## Key Questions\n",
    "- What doth R?\n",
    "- Call data from an existing set and use it to create custom urls\n",
    "- Webscraping tools and basic operation\n",
    "\n",
    "## Key Findings\n",
    "- R is wonderful\n",
    "- CRAN is my bu\n",
    "\n",
    "## Gameplan\n",
    "Here is my overall approach:\n",
    "1. Use R to go to the City and County of Honolulu's Real Property Assessment Division [website](https://www.realpropertyhonolulu.com/) to grab data about [agricultural dedications](https://www.realpropertyhonolulu.com/dedications/agricultural-dedications/) in the county.\n",
    "2. Download a pdf with tables of the . \n",
    "3. Parse the pdf for relevant data and compile the data into a more useful format (e.g., csv). \n",
    "4. Clean, if needed, the resulting file output\n",
    "5. Find a way to add the Tax Map Key (TMK, e.g., 290170150000) from csv file onto a url as a suffix to create a custom urls (e.g, http://qpublic9.qpublic.net/hi_honolulu_display.php?county=hi_honolulu&KEY=290170150000)\n",
    "6. Find a way to do this over and over\n",
    "7. Go to url with custom suffix \n",
    "8. Screap url more data that I want\n",
    "9. Add new data from url to existing dataset from pdf/csv\n",
    "\n",
    "*Stretch Goals*\n",
    "10. Geocode address data from pdf/csv\n",
    "11. Join with spatial dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Day 1 Work\n",
    "--\n",
    "#### Compiling Data on Multiple Excel Sheets\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/80/Seal_of_Honolulu%2C_Hawaii.svg/2000px-Seal_of_Honolulu%2C_Hawaii.svg.png\" alt=\"Drawing\" align=\"right\" style=\"width: 70px\" />\n",
    "\n",
    "\n",
    "-  Went to RPAD [agricultural dedications](https://www.realpropertyhonolulu.com/dedications/agricultural-dedications/) page via browers and manually downloaded the [2017 Dedicated Agricultural Parcels list](https://www.realpropertyhonolulu.com/media/1465/ag.pdf) pdf.\n",
    "- - - -\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://cdns.tblsft.com/sites/all/themes/tabwow/logo.png\" alt=\"Drawing\" align=\"right\" style=\"width: 100px\"/>\n",
    "\n",
    "-  Opened the pdf in [Tableau](https://www.tableau.com/) which has a built in [Data Interpreter](https://onlinehelp.tableau.com/current/pro/desktop/en-us/data_interpreter.html) designed to help clean data that converts and opens the pdf as an .xlsx file, but each page (of 33) was on a new sheet. I now needed a way to compile the data from the 34 sheets (minus the extra first sheet that has the Tableau interpreter key) into a single excel or csv sheet. \n",
    "\n",
    "----\n",
    "<img src=\"https://www.rstudio.com/wp-content/uploads/2017/05/readxl-259x300.png\" alt=\"Drawing\" align=\"right\" style=\"width: 70px;\" />\n",
    "\n",
    "- The [Readxl](http://readxl.tidyverse.org/index.html) R package is able to grab, fairly simply, from multiple sheets in either and .xsl or .xslx file.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'download' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9f3be41511dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Download file from url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdownload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://www.realpropertyhonolulu.com/media/1465/ag.pdf\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/Users/hunterheaivilin/agdedis.pdf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#Convert pdf into excel or csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'download' is not defined"
     ]
    }
   ],
   "source": [
    "# Download file from url \n",
    "download.file(\"https://www.realpropertyhonolulu.com/media/1465/ag.pdf\", \"/Users/hunterheaivilin/agdedis.pdf\")\n",
    "\n",
    "#Convert pdf into excel or csv\n",
    "\n",
    "\n",
    "# Install necessary packages\n",
    "install.packages(\"readxl\")\n",
    "install.packages(\"purrr\")\n",
    "\n",
    "#Call package libraries\n",
    "library(readxl)\n",
    "library(purrr)\n",
    "\n",
    "#Define file\n",
    "file <- 'path'\n",
    "\n",
    "# Make variable from exell sheets\n",
    "sheets <- excel_sheets(file)\n",
    "\n",
    "# Make dataframe from range of cells on multiple excel sheets \n",
    "df <- map_df(sheets, ~ read_excel(file, sheet = 2, range = \"A2:D45\"))\n",
    "\n",
    "write.csv(df, file = \"df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Day 2 Work\n",
    "--\n",
    "#### Downloading a File at a URL\n",
    "Used [Download.file](https://stat.ethz.ch/R-manual/R-devel/library/utils/html/download.file.html) to grab file from website.\n",
    "\n",
    "\n",
    "- - - -\n",
    "#### Extracting Data from Tables in a PDF\n",
    "\n",
    " - - - -\n",
    "<img src=\"http://blog.infographics.tw/wp-content/uploads/2015/06/cover4.jpg\" alt=\"Drawing\" align=\"right\" style=\"width: 200px;\"/>\n",
    "\n",
    "\n",
    "1. Tried to follow [Extracting Tables from PDFs in R using the Tabulizer Package](https://www.r-bloggers.com/extracting-tables-from-pdfs-in-r-using-the-tabulizer-package/), which shows how to use the [Tabulizer](https://github.com/ropensci/tabulizer) package for R ([tutorial](https://ropensci.org/tutorials/tabulizer_tutorial/)), but unfortunately \n",
    " \n",
    "> install.packages(\"tabulizer\")\n",
    "\n",
    "> Warning in install.packages :\n",
    "\n",
    "> package ‘tabulizer’ is not available (for R version 3.4.2)\n",
    "\n",
    "Apparently others have had [this issue](https://github.com/ropensci/tabulizer/issues/44).\n",
    "Fortunately there is an app version called [Tabula](http://tabula.technology/) that works quite well!\n",
    "\n",
    "** *Caveat:* ** The csv output retained the column headers from each page, meaning some further post processing is still needed.\n",
    "</br >\n",
    "- - - -\n",
    "<img src=\"https://pdftables.com/images/pdftables-logo.svg\" alt=\"Drawing\" align=\"right\" style=\"width: 200px;\"/>\n",
    "\n",
    "2. [PDFTables](https://pdftables.com/) also has an [R package](https://github.com/expersso/pdftables) with an API to the [web app](https://pdftables.com/), there is a sparse [CRAN documentation](https://cran.r-project.org/web/packages/pdftables/pdftables.pdf) and [tutorial](https://cran.r-project.org/web/packages/pdftables/vignettes/convert_pdf_tables.html)\n",
    "\n",
    "- - - - \n",
    "3. [pdftools](https://cran.r-project.org/web/packages/pdftools/pdftools.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Scraping with Various Tools\n",
    "\n",
    "**tl;dr:** Using old website is a considerable pain..\n",
    "Even when following a [handy tutorial](https://bradleyboehmke.github.io//2015/12/scraping-html-tables.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 37)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m37\u001b[0m\n\u001b[0;31m    .[2] %>%\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# Check for packages, install if necessary\n",
    "\n",
    "# Create list of packages needed for process\n",
    "list.of.packages <- c(\"rvest\", \"magrittr\")\n",
    "\n",
    "# Check list of needed packages against list of already installed packages and return those not install to new.packages variable\n",
    "new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,\"Package\"])]\n",
    "\n",
    "# Check new.packages variable for any data, if present, install new packages\n",
    "if(length(new.packages)) install.packages(new.packages)\n",
    "\n",
    "\n",
    "# Call package libraries\n",
    "library(rvest)\n",
    "library(magrittr)\n",
    "\n",
    "# Create variable with html of webpage\n",
    "webpage <- read_html(\"http://ecocrop.fao.org/ecocrop/srv/en/dataSheet?id=17807\")\n",
    "\n",
    "# Grab all the tables from the webpage\n",
    "tbls <- html_nodes(webpage, \"table\")\n",
    "\n",
    "\n",
    "# Or, since none of the tables have unique identifiers (<table> for all!) you can\n",
    "# create empty list to add table data to\n",
    "tbls2_ls <- list()\n",
    "\n",
    "# then specify which table(s) you want to grab & name them something useful (e.g., Ecology, ... , Uses)\n",
    "tbls2_ls$Description <- webpage %>%\n",
    "     html_nodes(\"table\") %>% \n",
    "        .[1] %>%\n",
    "     html_table(fill = TRUE) %>%\n",
    "     .[[1]]\n",
    "\n",
    "tbls2_ls$Ecology <- webpage %>%\n",
    "     html_nodes(\"table\") %>% \n",
    "    .[2] %>%\n",
    "    html_table(fill = TRUE) %>%\n",
    "     .[[1]]\n",
    "\n",
    "tbls2_ls$ClimaticZone <- webpage %>%\n",
    "     html_nodes(\"table\") %>% \n",
    "    .[3] %>%\n",
    "     html_table(fill = TRUE) %>%\n",
    "     .[[1]]\n",
    " \n",
    "tbls2_ls$Cultivation <- webpage %>%\n",
    "     html_nodes(\"table\") %>% \n",
    "    .[4] %>%\n",
    "     html_table(fill = TRUE) %>%\n",
    "     .[[1]]\n",
    "\n",
    "tbls2_ls$Uses <- webpage %>%\n",
    "     html_nodes(\"table\") %>% \n",
    "    .[6] %>%\n",
    "     html_table(fill = TRUE) %>%\n",
    "     .[[1]]\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-cf6aa2744b11>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-cf6aa2744b11>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    colnames(tbls2_ls$Uses) <- tbls2_ls$Uses[1,]\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Clean up list tables into a better format\n",
    "\n",
    "# Rename columns with variables in first row\n",
    "colnames(tbls2_ls$Uses) <- tbls2_ls$Uses[1,]\n",
    "\n",
    "#Remove first row\n",
    "tbls2_ls$Uses <- tbls2_ls$Uses[-1,]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Peer Feedback on Day 3\n",
    "\n",
    "After talking it over with a peer, I received the following feedback and decided to make these changes\n",
    "\n",
    "## Here are some overall notes on the skills I learned\n",
    "And perhaps some stream of consciousness notes about what I did, and other questions I might have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping from HTML tables into R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-e06ded7b64df>, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-e06ded7b64df>\"\u001b[0;36m, line \u001b[0;32m20\u001b[0m\n\u001b[0;31m    for(i in 1:nrow(truncated)) {\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Grab list of crop variables from a csv\n",
    "item_list <- read.csv(\"/Users/hunterheaivilin/Downloads/2num.csv\", header = FALSE)\n",
    "izem <- data.frame(item_list[1,])\n",
    "\n",
    "# Grab list of crop codes and urls\n",
    "urllists <- read.csv(\"/Users/hunterheaivilin/Downloads/datasheeturl.csv\")\n",
    "\n",
    "#Create variable of all urls\n",
    "webpages <- (urllists)\n",
    "\n",
    "truncated <- data.frame(urllists[1:3,])\n",
    "\n",
    "# Grab list of crop variables from a csv\n",
    "item_list <- read.csv(\"/Users/hunterheaivilin/Downloads/2num.csv\", header = FALSE)\n",
    "\n",
    "\n",
    "\n",
    "# for loop to move through\n",
    "for(i in 1:nrow(truncated)) {\n",
    "  url <- toString(truncated[i,1])\n",
    "  html <- read_html(url)\n",
    "\n",
    "  #Grab the species name\n",
    "  species <- html_text(html_nodes(html, \"h2\"))\n",
    "  \n",
    "  # Create empty list to add table data into\n",
    "  tbls2_ls <- list()\n",
    "  \n",
    "  # Specify which table(s) from html you want to grab & name them something useful (e.g., Ecology, ... , Uses)\n",
    "  tbls2_ls$Description <- html %>%\n",
    "    html_nodes(\"table\") %>% \n",
    "    .[1] %>%\n",
    "    html_table(fill = TRUE) %>%\n",
    "    .[[1]]\n",
    "  \n",
    "  tbls2_ls$Ecology <- html %>%\n",
    "    html_nodes(\"table\") %>% \n",
    "    .[2] %>%\n",
    "    html_table(fill = TRUE) %>%\n",
    "    .[[1]]\n",
    "  \n",
    "  tbls2_ls$ClimaticZone <- html %>%\n",
    "    html_nodes(\"table\") %>% \n",
    "    .[3] %>%\n",
    "    html_table(fill = TRUE) %>%\n",
    "    .[[1]]\n",
    "  \n",
    "  tbls2_ls$Cultivation <- html %>%\n",
    "    html_nodes(\"table\") %>% \n",
    "    .[4] %>%\n",
    "    html_table(fill = TRUE) %>%\n",
    "    .[[1]]\n",
    "  \n",
    "  tbls2_ls$Uses <- html %>%\n",
    "    html_nodes(\"table\") %>% \n",
    "    .[6] %>%\n",
    "    html_table(fill = TRUE) %>%\n",
    "    .[[1]]\n",
    "  \n",
    "  \n",
    "  #Clean up 'Uses' table into a better format\n",
    "  # Rename columns with variables in first row\n",
    "  colnames(tbls2_ls$Uses) <- tbls2_ls$Uses[1,]\n",
    "  \n",
    "  #Remove first row\n",
    "  tbls2_ls$Uses <- tbls2_ls$Uses[-1,]\n",
    "  \n",
    "# Assign variables from table data\n",
    "  \n",
    "  \n",
    "  # Creates variables that concide with item_list \n",
    "  \n",
    "  c1 <- \"crop code should go here\"\n",
    "  c2 <- species\n",
    "  c3 <- tbls2_ls$Description[1, 2]\n",
    "  c4 <- tbls2_ls$Description[2, 2]\n",
    "  c5 <- tbls2_ls$Description[3, 2]\n",
    "  c6 <- tbls2_ls$Description[1, 4]\n",
    "  c7 <- tbls2_ls$Description[2, 4]\n",
    "  c8 <- tbls2_ls$Description[3, 4]\n",
    "  c9 <- tbls2_ls$Ecology[3,2]\n",
    "  c10 <- tbls2_ls$Ecology[3,3]\n",
    "  c11 <- tbls2_ls$Ecology[3,4]\n",
    "  c12 <- tbls2_ls$Ecology[3,5]\n",
    "  c13 <- tbls2_ls$Ecology[4,2]\n",
    "  c14 <- tbls2_ls$Ecology[4,3]\n",
    "  c15 <- tbls2_ls$Ecology[4,4]\n",
    "  c16 <- tbls2_ls$Ecology[4,5]\n",
    "  c17 <- tbls2_ls$Ecology[5,2]\n",
    "  c18 <- tbls2_ls$Ecology[5,3]\n",
    "  c19 <- tbls2_ls$Ecology[5,4]\n",
    "  c20 <- tbls2_ls$Ecology[5,5]\n",
    "  c21 <- tbls2_ls$Ecology[6,2]\n",
    "  c22 <- tbls2_ls$Ecology[6,3]\n",
    "  c23 <- tbls2_ls$Ecology[6,4]\n",
    "  c24 <- tbls2_ls$Ecology[6,5]\n",
    "  c25 <- tbls2_ls$Ecology[7,2]\n",
    "  c26 <- tbls2_ls$Ecology[7,3]\n",
    "  c27 <- tbls2_ls$Ecology[7,4]\n",
    "  c28 <- tbls2_ls$Ecology[7,5]\n",
    "  c29 <- tbls2_ls$Ecology[8,2]\n",
    "  c30 <- tbls2_ls$Ecology[8,3]\n",
    "  c31 <- tbls2_ls$Ecology[8,4]\n",
    "  c32 <- tbls2_ls$Ecology[8,5]\n",
    "  c33 <- tbls2_ls$Ecology[2,7]\n",
    "  c34 <- tbls2_ls$Ecology[2,8]\n",
    "  c35 <- tbls2_ls$Ecology[3,7]\n",
    "  c36 <- tbls2_ls$Ecology[3,8]\n",
    "  c37 <- tbls2_ls$Ecology[4,7]\n",
    "  c38 <- tbls2_ls$Ecology[4,8]\n",
    "  c39 <- tbls2_ls$Ecology[5,7]\n",
    "  c40 <- tbls2_ls$Ecology[5,8]\n",
    "  c41 <- tbls2_ls$Ecology[6,7]\n",
    "  c42 <- tbls2_ls$Ecology[6,8]\n",
    "  c43 <- tbls2_ls$Ecology[7,7]\n",
    "  c44 <- tbls2_ls$Ecology[7,8]\n",
    "  c45 <- tbls2_ls$ClimaticZone[1,2]\n",
    "  c46 <- tbls2_ls$ClimaticZone[1,4]\n",
    "  c47 <- tbls2_ls$ClimaticZone[2,2]\n",
    "  c48 <- tbls2_ls$ClimaticZone[2,4]\n",
    "  c49 <- tbls2_ls$ClimaticZone[3,2]\n",
    "  c50 <- tbls2_ls$ClimaticZone[3,4]\n",
    "  c51 <- tbls2_ls$ClimaticZone[4,2]\n",
    "  c52 <- tbls2_ls$Cultivation[2,2]\n",
    "  c53 <- tbls2_ls$Cultivation[3,1]\n",
    "  c54 <- tbls2_ls$Cultivation[3,2]\n",
    "  c55 <- tbls2_ls$Cultivation[3,3]\n",
    "  c56 <- tbls2_ls$Cultivation[3,4]\n",
    "  c57 <- tbls2_ls$Cultivation[3,5]\n",
    "  c58 <- tbls2_ls$Cultivation[2,4]\n",
    "  c59 <- tbls2_ls$Cultivation[2,5]\n",
    "  c60 <- c(tbls2_ls$Uses [1,1])\n",
    "  c61 <- c(tbls2_ls$Uses[1,2])\n",
    "  c62 <- c(tbls2_ls$Uses[1,3])\n",
    "  c63 <- url\n",
    "  \n",
    "  # Make a big 'ol list\n",
    "  crop_data <- list (c1, c2, c3, c4 ,c5, c6, c7, c8, c9, c10, c11, c12, c13, c14, c15, c16, c17, c18, c19, c20, c21, c22, c23, c24, c25, c26, c27, c28, c29, c30, c31, c32, c33, c34, c35, c36, c37, c38, c39, c40, c41, c42, c43, c44, c45, c46, c47, c48, c49, c50, c51, c52, c53, c54, c55, c56, c57, c58, c59, c60, c61, c62, c63)\n",
    "  \n",
    " super <- data.frame(crop_data)\n",
    "\n",
    "  # Transpose\n",
    "  crop_data <- t(crop_data)\n",
    "  \n",
    "  print(crop_data)\n",
    "  \n",
    "  #convert to data frame\n",
    "  crop_df <- data.frame(crop_data)\n",
    "  \n",
    "\n",
    "\n",
    "# attach ____ as a row in dataframe  \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "# Read html from each url into tables\n",
    "newb <- newdf\n",
    "\n",
    "# parse and return multiple grabs as a ____\n",
    "\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
