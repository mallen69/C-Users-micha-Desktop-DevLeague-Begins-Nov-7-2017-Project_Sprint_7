{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Sampling, Instruments, and Bias\n",
    "*Module: Experimental Design (Sprint 1 of 2)*\n",
    "\n",
    "*Experiments and the scientific method are at the heart of how we “know” what we know when it comes to data analysis. But how does it translate to the different situations we encounter in practice and what are some common pitfalls to be aware of?*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Data Journalist| Data Engineer | Statistical Modeler| Business Analyst |\n",
    "|:----------------:|:----:|:------------------:|:----:|\n",
    "|I need to **understand the different ways to study sample populations and the potential biases introduced** so that I can assess the value of published research into my investigations|I need to be able to **implement valid sampling and collection procedures** for data at all scales so that I can support analyses without inadvertently introducing bias|I need to **understand how sampling and instruments introduce bias** so that I can design analyses that account for them|I need to **design effective data collection instruments** so that I can answer critical questions for my business|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytical Process Big Picture\n",
    "![Curriculum Summary](../curriculum_summary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When we formulate good questions...\n",
    "- we need to know what a questions can be answered by what evidence\n",
    "- what kind of evidence we can gather\n",
    "- how we gather evidence\n",
    "- what can our collection tell us about how we interpret the answer\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Key Questions\n",
    "- How do we interpret evidence?\n",
    "- What is the goal of data analysis?\n",
    "- What is the goal of an experiment? How is an experiment different from simply observing nature?\n",
    "- How do we establish causality?\n",
    "- How do the things we measure relate to the things we want to know?\n",
    "- How are surveys like sensors?\n",
    "- Why is a random sample important, what abilities does it afford us?\n",
    "- How do know that a sample represents a population?\n",
    "- How do we guarantee a random sample and what do we do if we can't?\n",
    "\n",
    "## Key Concepts and Definitions\n",
    "- scientific method\n",
    "- analytical method\n",
    "- experimental study\n",
    "- observational study\n",
    "- evidence\n",
    "- experimental evidence\n",
    "- anecdotal evidence\n",
    "- observational evidence\n",
    "- causality\n",
    "- representative sample\n",
    "- outcome / dependent variable\n",
    "- explantory / independent / predictor variable\n",
    "- simple random sampling\n",
    "- stratified sampling\n",
    "- cluster sampling\n",
    "- systematic sampling\n",
    "- convenience sample\n",
    "- voluntary response sample\n",
    "- central limit theorem\n",
    "- scale / response formats\n",
    "- likert scale\n",
    "- multiple response scale\n",
    "- nominal variable\n",
    "- ordinal variable\n",
    "- ranking\n",
    "- research bias\n",
    "- response bias\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Themes of this Sprint\n",
    "- Evidence / Experiments / Observation\n",
    "- Causality and Relationships\n",
    "- Variables\n",
    "- Instrument\n",
    "- Survey Design\n",
    "- Samples / CLT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge\n",
    "\n",
    "This is the module that connects this exercise in analysis with knowledge. Broadly. How do we know anything. What do we know. What role does the analytical process and data and evidence play in that\n",
    "\n",
    "From Experimental Design PDF\n",
    ">\"Experimental Design and Statistical Analysis go hand in hand, and neither can be understood without the other.\" \n",
    "\n",
    "\n",
    ">\"I need to say a few things about the difficulties of learning about experi- mental design and analysis. A practical working knowledge requires understanding many concepts and their relationships. Luckily much of what you need to learn agrees with common sense, once you sort out the terminology. On the other hand, there is no ideal logical order for learning what you need to know, because every- thing relates to, and in some ways depends on, everything else. So be aware: many concepts are only loosely defined when first mentioned, then further clarified later when you have been introduced to other related material. Please try not to get frustrated with some incomplete knowledge as the course progresses. If you work hard, everything should tie together by the end of the course.\"\n",
    "\n",
    "## Variables\n",
    "\n",
    "manipulate one variable and observe the effects on another\n",
    "\n",
    "\n",
    "## Related / Correlation / Causation\n",
    ">If variables X and Y (e.g., the number of televisions (X) in various countries and the infant mortality rate (Y) of those countries) are found to be associated, then there are three basic possibilities. \n",
    "- First X could be causing Y (televisions lead to more health awareness, which leads to better prenatal care) \n",
    "- or Y could be causing X (high infant mortality leads to attraction of funds from richer countries, which leads to more televisions) \n",
    "- or unknown factor Z could be causing both X and Y (higher wealth in a country leads to more televisions and more prenatal care clinics). \n",
    "\n",
    ">It is worth memorizing these three cases, because they should always be considered when association is found in an observational study as opposed to a randomized experiment. (It is also possible that X and Y are related in more complicated ways including in large networks of variables with feedback loops.)\n",
    "\n",
    ">Causation (“X causes Y”) can be logically claimed if X and Y are associated, and X precedes Y, and no plausible alternative explanations can be found, par- ticularly those of the form “X just happens to vary along with some real cause of changes in Y” (called confounding)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Design\n",
    "http://www.statisticshowto.com/experimental-design/\n",
    "    \n",
    "> Experimental design is a way to carefully plan experiments in advance so that your results are both objective and valid. The terms “Experimental Design” and “Design of Experiments” are used interchangeably and mean the same thing. However, the medical and social sciences tend to use the term “Experimental Design” while engineering, industrial and computer sciences favor the term “Design of experiments.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirmatoratory vs Exploratory Research \n",
    "https://en.wikipedia.org/wiki/Research_design\n",
    "\n",
    ">Confirmatory research tests a priori hypotheses — outcome predictions that are made before the measurement phase begins. Such a priori hypotheses are usually derived from a theory or the results of previous studies. The advantage of confirmatory research is that the result is more meaningful, in the sense that it is much harder to claim that a certain result is generalizable beyond the data set. The reason for this is that in confirmatory research, one ideally strives to reduce the probability of falsely reporting a coincidental result as meaningful. This probability is known as α-level or the probability of a type I error.\n",
    "\n",
    ">Exploratory research on the other hand seeks to generate a posteriori hypotheses by examining a data-set and looking for potential relations between variables. It is also possible to have an idea about a relation between variables but to lack knowledge of the direction and strength of the relation. If the researcher does not have any specific hypotheses beforehand, the study is exploratory with respect to the variables in question (although it might be confirmatory for others). The advantage of exploratory research is that it is easier to make new discoveries due to the less stringent methodological restrictions. Here, the researcher does not want to miss a potentially interesting relation and therefore aims to minimize the probability of rejecting a real effect or relation; this probability is sometimes referred to as β and the associated error is of type II. In other words, if the researcher simply wants to see whether some measured variables could be related, he would want to increase the chances of finding a significant result by lowering the threshold of what is deemed to be significant.\n",
    "\n",
    ">Sometimes, a researcher may conduct exploratory research but report it as if it had been confirmatory ('Hypothesizing After the Results are Known', HARKing—see Hypotheses suggested by the data); this is a questionable research practice bordering on fraud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensor Design\n",
    "\n",
    "https://www.clear.rice.edu/elec201/Book/sensors.html\n",
    "    \n",
    "> Without sensors, a robot is just a machine. Robots need sensors to deduce what is happening in their world and to be able to react to changing situations. This chapter introduces a variety of robotic sensors and explains their electrical use and practical application. The sensor applications presented here are not meant to be exhaustive, but merely to suggest some of the possibilities. Please do not be limited by the ideas contained in this chapter! Assembly instructions for the kit sensors are given in Section 2.6.\n",
    "\n",
    "\n",
    "Sensors as Transducers (and Data Collectors)  \n",
    "\n",
    "> The basic function of an electronic sensor is to measure some feature of the world, such as light, sound, or pressure and convert that measurement into an electrical signal, usually a voltage or current. Typical sensors respond to stimuli by changing their resistance (photocells), changing their current flow (phototransistors), or changing their voltage output (the Sharp IR sensor). The electrical output of a given sensor can easily be converted into other electrical representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on Experimental Design\n",
    "EXPERIMENTAL DESGIN BOOK PDF\n",
    "\n",
    ">Experimental design is a careful balancing of several features including ** “power”, generalizability, various forms of “validity”, practicality and cost**. These concepts will be defined and discussed thoroughly in the next chapter. For now, you need to know that often an improvement in one of these features has a detrimental effect on other features. \n",
    "\n",
    "> A thoughtful balancing of these features in advance will result in an experiment with the ** best chance of providing useful evidence ** to modify the current state of knowledge in a particular scientific field. On the other hand, it is unfortunate that many experiments are designed with avoidable flaws. It is only rarely in these circumstances that statistical analysis can rescue the experimenter. This is an example of the old maxim “an ounce of prevention is worth a pound of cure”.\n",
    "\n",
    ">Our goal is always to actively design an experiment that has the best chance to produce meaningful, defensible evidence, rather than hoping that good statistical analysis may be able to correct for defects after the fact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Design and EDA\n",
    "> Statistical analysis of experiments starts with graphical and non-graphical ex- ploratory data analysis (EDA). EDA is useful for\n",
    "• detection of mistakes\n",
    "• checking of assumptions\n",
    "• determining relationships among the explanatory variables\n",
    "• assessing the direction and rough size of relationships between explanatory and outcome variables, and\n",
    "\n",
    "## Experimental Design and Modeling relationships\n",
    "\n",
    ">Most formal (confirmatory) statistical analyses are based on models. Statis- tical models are ideal, mathematical representations of observable characteristics. Models are best divided into two components. **The structural component of the model (or structural model) specifies the relationships between explana- tory variables and the mean (or other key feature) of the outcome variables**. The **“random” or “error” component of the model (or error model) characterizes the deviations of the individual observations from the mean**. (Here, “error” does not indicate “mistake”.) The two model components are also called **“signal” and “noise”** respectively. \n",
    "\n",
    ">Statisticians realize that no mathematical models are perfect representations of the real world, but some are close enough to reality to be useful. A full description of a model should include all assumptions being made because statistical inference is impossible without assumptions, and sufficient deviation of reality from the assumptions will invalidate any statistical inferences.\n",
    "\n",
    ">A slightly different point of view says that models describe how the distribution of the outcome varies with changes in the explanatory variables.\n",
    "\n",
    " \n",
    "> **Statistical models have both a structural component and a random component which describe means and the pattern of deviation from the mean, respectively.**\n",
    "\n",
    "\n",
    "## VARIABLE SELECTION\n",
    "> Operational- izations define measures or variables which are quantities of interest or which serve as the practical substitutes for the concepts of interest. For example, if you have a theory about what affects people’s anger level, you need to operationalize the concept of anger. \n",
    "\n",
    "\n",
    "## What makes a “good” variable?\n",
    "> Regardless of what we are trying to measure, the qualities that make a good measure of a scientific concept are high reliability, absence of bias, low cost, prac- ticality, objectivity, high acceptance, and high concept validity. \n",
    "> **Reliability** is essentially the inverse of the statistical concept of variance, and a rough equivalent is “consistency”. Statisticians also use the word “precision”.\n",
    "\n",
    "> **Bias** refers to the difference between the measure and some “true” value. A difference between an individual measurement and the true value is called an “er- ror” (which implies the practical impossibility of perfect precision, rather than the making of mistakes). The bias is the average difference over many measurements. Ideally the bias of a measurement process should be zero. For example, a mea- sure of weight that is made with people wearing their street clothes and shoes has a positive bias equal to the average weight of the shoes and clothes across all subjects.\n",
    "\n",
    "> All other things being equal, when two measures are available, we will choose the **less expensive and easier to obtain (more practical) measures**. Measures that have a greater degree of subjectivity are generally less preferable. Although devising your own measures may improve upon existing measures, there may be a trade off with acceptability, resulting in reduced impact of your experiment on the field as a whole.\n",
    "\n",
    "> Construct validity is a key criterion for variable definition. Under ideal conditions, after completing your experiment you **will be able to make a strong claim that changing your explanatory variable(s) in a certain way (e.g., doubling the amplitude of a background hum) causes a corresponding change in your out- come (e.g., score on an irritability scale)**. But if you want to convert that to meaningful statements about the effects of auditory environmental disturbances on the psychological trait or construct called “irritability”, you must be able to argue that the scales have good construct validity for the traits, namely that the **operationalization of background noise as an electronic hum has good construct validity for auditory environmental disturbances**, and that your **irritability scale really measures what people call irritability**. Although construct validity is critical to the impact of your experimentation, its detailed understanding belongs sepa- rately to each field of study, and will not be discussed much in this book beyond the discussion in Chapter 3.\n",
    "\n",
    "## Construct Validity\n",
    "> Construct validity is a characteristic of devised measurements that describes how well the measurement can stand in for the scientific concepts or “constructs” that are the real targets of scientific learning and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instrumentation\n",
    "https://www.quora.com/What-is-Definition-of-Instrumentation\n",
    "> Instrumentation is the variety of measuring instruments to monitor and control a process. It is the art and science of measurement and control of process variables within a production, laboratory, or manufacturing area.\n",
    "\n",
    "> Instrumentation is defined as the art and science of measurement and control of process variables within a production or manufacturing area. The process variables used in industries are Level, Pressure, Temperature, Humidity, Flow, pH, Force, Speed etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survey Design\n",
    "https://en.wikiversity.org/wiki/Survey_design\n",
    "\n",
    "> This learning resource is about how to design surveys (or questionnaires) in the social sciences.\n",
    "\n",
    "> Surveys are commonly used in disciplines such as psychology, health, marketing, sociology, governance, and demographics.\n",
    "\n",
    "> Survey research is an efficient way of gathering data to help address a research question. The main challenge is developing reliable and valid measures and sampling representative data.\n",
    "\n",
    "> Survey design is critical in determining the quality of research. The potential for poor design is vast - whether intentionally on the part of the researcher or unintentionally. For example, watch this [http://www.youtube.com/watch?v=G0ZZJXw4MTA 2 min. episode of Yes, Minister] about politicians trying to get the poll results they want.\n",
    "\n",
    ">Before designing a survey[edit]\n",
    ">It can be very tempting to press ahead with designing a survey. But first, be clear about the purpose of the study and the research methodology.\n",
    "\n",
    "\n",
    ">Designing a survey? Don't put the cart before the horse. Develop a proposal first, then design the survey.\n",
    "Before designing a survey, develop a research proposal which clearly explains the:\n",
    "- research purpose\n",
    "- research questions\n",
    "- hypotheses\n",
    "- Research design: Experimental, quasi-experimental, non-experimental\n",
    "- Sampling method\n",
    "- Target constructs - operationally define the:\n",
    "- independent variables\n",
    "- dependent variables\n",
    "\n",
    "\n",
    "> Types of questions[edit]\n",
    "It is surprisingly difficult to develop a \"good\" survey question or item. Consider each of the following aspects of survey questions, their pros and cons, and with examples:\n",
    "\n",
    ">Objective vs. subjective\n",
    "- Close-ended vs. open-ended\n",
    "- Leading and loaded questions\n",
    "- Positive-, negative-, and double-negative-wording\n",
    "\n",
    "\n",
    "> Response formats[edit]\n",
    "It is important to understand the implications of response formats on levels of measurement in survey design and quantitative data analysis.\n",
    "\n",
    ">Some commonly used response formats include:\n",
    "- Dichotomous: e.g., Yes or No\n",
    "- Multi-chotomous: e.g., Yes, No, or Maybe\n",
    "- Multiple response: e.g., Tick all that apply\n",
    "- Likert scale: Equally-spaced intervals, usually 3 to 9 intervals\n",
    "- Graphical rating: Can mark any point on a continuous scale\n",
    "- Ranking: Compare items to each other by placing them in order of descending preference\n",
    "- Semantic differential: Put two words at opposite ends of a scale with interval marks\n",
    "- Idiographic: Use symbols/pictures instead of words and numbers\n",
    "- For more info see: Rating scale (Wikipedia)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on Survey design\n",
    "http://www.esourceresearch.org/eSourceBook/SampleSurveys/6DevelopingaSurveyInstrument/tabid/484/Default.aspx\n",
    "    \n",
    "> In general, survey questions should:\n",
    "- Contain only one idea or question\n",
    "- Define the scope to consider, such as the time period or activities that are relevant to the question\n",
    "- Be written with neutral language to avoid leading the respondent to a specific answer\n",
    "- Use language that enables less educated persons to easily understand the question.\n",
    "- Contain response options that are simple, clear, consistent, and include the full range of responses that might occur\n",
    "- For categorical responses, be mutually exclusive and exhaustive so that a respondent can pick one and only one option\n",
    "- For numeric responses, guide the respondent to provide the response in a consistent format and units "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Errors and Biases in Survey Research\n",
    "\n",
    "https://blog.cruxresearch.com/2013/08/27/the-top-5-errors-and-biases-in-survey-research/\n",
    "\n",
    ">The Top 5\n",
    "\n",
    ">1.  Researcher Bias.\n",
    "\n",
    ">The most important error that creeps into surveys about isn’t statistical at all and is not measurable. The viewpoint of the researcher has a way of creeping into question design and analysis. Some times this is purposeful, and other times it is more subtle. All research designers are human, and have points-of-view. Even the most practiced and professional researchers can have subtle biases in the way they word questions or interpret results. How we frame questions and report results is always affected by our experiences and viewpoints – which can be a good thing, but can also affect the purity of the study.\n",
    "\n",
    ">2. Poor match of the sample to the population.\n",
    "\n",
    ">This is the source of some of the most famous errors in polling. Our industry once predicted the elections of future Presidents Alf Landon and Thomas Dewey based on this mistake. It is almost never the case that the sampling frame you use is a perfect match to the population you are trying to understand, so this error is present on most studies. You can sometimes recover from asking the wrong questions, but you can never recover from asking them of the wrong people\n",
    "\n",
    ">Most clients (and suppliers) like to focus on questionnaire development when a new project is awarded. The reality is the sampling and weighting plan is every bit as consequential to the success of the project, and rarely gets the attention it deserves. We can tell when we have a client that really knows what they are doing if they begin the project by focusing on sampling issues and not jumping to questionnaire design.\n",
    "\n",
    ">3. Lack of randomness/response bias.\n",
    "\n",
    ">Many surveys proceed without random samples. In fact, it is rare that a survey being done today can accurately claim to be using a random sample. Remember those statistics courses you took in college and graduate school? The one thing they have in common is pretty much everything they taught you statistically is only relevant if you have a random sample. And, odds are great that you don’t.\n",
    "\n",
    ">A big source of “non-randomness” in a sample is response bias. A typical RDD phone survey being conducted today has a cooperation rate of less than 20%. 10% is considered a good response rate from an online panel. When we report results of these studies, we are assuming that the vast majority of people who didn’t respond would have responded in the same way as those who did. Often, this is a reasonable assumption. But, sometimes it is not. Response bias is routinely ignored in market research and polls because it is expensive to correct (the fix involves surveying the non-responders).\n",
    "\n",
    ">4.  Failure to quota sample or weight data.\n",
    "\n",
    ">This is a bit technical. Even if we sample randomly, it is typical for some subgroups to be more willing to cooperate than others. For example, females are typically less likely to refuse a survey invitation than males, and minorities are less likely to participate than whites. So, a good researcher will quota sample and weight data to compensate for this. In short, if you know something about your population before you survey them, you should use this knowledge to your advantage. If you are conducting an online poll and you are not doing something to quota sample or weight the data, odds are very good that you are making an important mistake.\n",
    "\n",
    ">5.  Overdoing it.\n",
    "\n",
    ">I have worked with methodologists who have more degrees than a thermometer, think about the world in Greek letters, and understand every type of bias we can comprehend. I have also seen them concentrate so much on correcting for every type of error they can imagine that they “overcook” the data. I remember once passing off a data set to a statistician, who corrected for 10 types of errors, and the resulting data set didn’t even have the gender distribution it the proper proportion.\n",
    "\n",
    ">Remember — you don’t have to correct for an error or bias unless it has an effect on what you are asking.  For example, if men and women answer a question identically, weighting by gender will have no effect on the study results. Instead, you should know enough about the issues you are studying to know what types of errors are likely to be relevant to your study.\n",
    "\n",
    ">So that is our top 5. Note that I did not put sampling error in the top 5. I am not sure it would make my top 20. Sampling error is the “+/- 5%” that you see attached to many polls. We will do a subsequent blog post on why this isn’t a particularly relevant error for most studies. It just happens to be the one type of error that can be easily calculated mathematically, which is why we see it cited so often. I am more concerned about the errors that are harder to calculate, or, more importantly, the ones that go unnoticed.\n",
    "\n",
    ">With 40+ sources of errors, one could wonder how our industry ever gets it right. Yet we do. More than $10 Billion is spent on research and polling in the US each year, and if this money was not being spent effectively, the industry would implode. So, how do we get it right?\n",
    "\n",
    ">In one sense, many of the errors in surveys tend to be randomly distributed. For instance, there can be a fatigue bias in a question involving a long list of items to be assessed. By presenting long lists in a randomized order we can “randomize” this error – we don’t remove it.\n",
    "\n",
    ">In some sense, errors and biases also seem to have a tendency to cancel each other out, rather than magnify each other. And, as stated above, not all errors matter to every project. The key is to consider which ones might before the study is fielded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Sampling types\n",
    "https://www.khanacademy.org/math/statistics-probability/designing-studies/sampling-methods-stats/a/sampling-methods-review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Internal Validity (concept for Next Sprint)\n",
    "http://www.indiana.edu/~educy520/sec5982/week_9/520in_ex_validity.pdf\n",
    "\n",
    ">Why is Internal Validity Important?\n",
    "We often conduct research in order to determine\n",
    "cause-and-effect relationships.\n",
    "■ Can we conclude that changes in the independent\n",
    "variable caused the observed changes in the\n",
    "dependent variable?\n",
    "■ Is the evidence for such a conclusion good or poor?\n",
    "■ If a study shows a high degree of internal validity then\n",
    "we can conclude we have strong evidence of\n",
    "causality.\n",
    "■ If a study has low internal validity, then we must\n",
    "conclude we have little or no evidence of causality.\n",
    "\n",
    "\n",
    "# Necessary Conditions for Causality\n",
    ">Three conditions that are necessary to claim that\n",
    "variable A causes changes in variable B:\n",
    "• Relationship condition: Variable A and variable B\n",
    "must be related.\n",
    "• Temporal Antecedence condition: Proper time order\n",
    "must be established.\n",
    "• Lack of Alternative Explanation Condition:\n",
    "Relationship between variable A and variable B\n",
    "must not be attributable to a confounding,\n",
    "extraneous variable.\n",
    "\n",
    "\n",
    ">Threats to internal validity compromise our confidence\n",
    "in saying that a relationship exists between the\n",
    "independent and dependent variables.\n",
    "\n",
    ">Threats to external validity compromise our\n",
    "confidence in stating whether the study’s results are\n",
    "applicable to other groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Project Ideas\n",
    "\n",
    "From Justin for Both Sprints on Experimental Design and Research Methods\n",
    "- What are you trying to understand? What questions are you trying to answer?\n",
    "- How is data better than your intuition or ‘gut’?\n",
    "- Is the thing you want to understand directly measureable? \n",
    "- If not, what are some proxies to that? \n",
    "- What is correlation vs. causation?\n",
    "- What might confound your ability to approximate?\n",
    "- Are there reasonable ranges of values you may expect to encounter?\n",
    "- Are there unreasonable ranges? How do you know?\n",
    "- Pull the measureable or proxy-based data and visualize it. \n",
    "        - What does it tell you? \n",
    "        - What are some basic conclusions?\n",
    "        - What are some reasons that this may be wrong?\n",
    "        - How can you overcome these limitations?\n",
    "- Talk about Thomas Kuhn, Karl Popper, Michael Polanyi. \n",
    "- Tools: Scipy, numpy, PyMC, statsmodels, sci-kit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
