{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data Operations - Sprint Journal for Natalie Ramirez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "## DAY 1: Tuesday (Week 1)\n",
    "### What I Expect to Learn\n",
    "\n",
    "I want to learn how to automate the process of exporting aggregating data into a readable file to a designated folder in google drive. I hope to determine which tools best fit the needs of my data analysis.\n",
    "\n",
    "Key Questions:\n",
    "- What are the differences between tables and dataframes?\n",
    "- What are the differences between lists and arrays?\n",
    "- If my data is already modeled and in my database how am I going to companre an external data source for duplicates? Will I have to aggregate data all in one table or will I have another data dump table that just has everything and use that to compare?\n",
    "\n",
    "### Project References\n",
    "- [RAM Cloud Project](https://ramcloud.atlassian.net/wiki/spaces/RAM/pages/6848531/Data+Operations)\n",
    "- [Comparing two or more data frames and find rows that appear in more than one data frame, or rows that appear only in one data frame.](http://www.cookbook-r.com/Manipulating_data/Comparing_data_frames/)\n",
    "- [Comparing matching data structures in Tableau](http://kb.tableau.com/articles/howto/comparing-matching-and-non-matching-records-across-blended-data-sources)\n",
    "\n",
    "### Project Pitch\n",
    "- In this sprint I will compare data from different data sources for duplicate records in R and then upload unique records to a PostgreSQL database.\n",
    "- This project will help my aggregate, normalize, and structure data using data operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## DAY 2: Wednesday (Week 1)\n",
    "### Pitch Feedback \n",
    "- Use R to compare records.\n",
    "- Make sure to normalize data to create data model.\n",
    "\n",
    "### Prototype Notes\n",
    "*Results from prototype experiments, snippets of code, things I tried * \n",
    "\n",
    "### Pair Show & Tell Comments\n",
    "*Comments from prototype discussion*\n",
    "- Can use the cookbook-r as resource\n",
    "- Can complete datacamp R intro course\n",
    "- Can also try to remove duplicates straight in SQL.\n",
    "\n",
    "### Proposed Plan: Key Milestones by Day\n",
    "\n",
    "##### Day 2 (Wed Week 1):\n",
    "- Develop Project Proposal\n",
    "- Push Docs / Repo / Roadmap Update\n",
    "\n",
    "##### Day 3 (Thu Week 1):\n",
    "- Normalize Data\n",
    "- Generate Data Model\n",
    "- Create Dups Script\n",
    "- Push docs / repo\n",
    "\n",
    "##### Day 4 (Tue Week 2): \n",
    "- Fix Python Script to collect new data\n",
    "- Update Python Script to insert data into SQL Database\n",
    "- Push docs / repo\n",
    "\n",
    "##### Day 5 (Wed Week 2):\n",
    "- Project highlights\n",
    "- Identify question or cohort knowledge gap for sprint review\n",
    "- Develop Topic Project + Presentation\n",
    "- Push Repo / docs / Presentation\n",
    "\n",
    "### Project Definition and README.MD Discussion \n",
    "*This is a discussion of how this project will fit into my overall roadmap. I will update my roadmap with the following project definition*\n",
    "\n",
    "*I will focus my project Repo's README.MD on the same topic, but with this additional detail.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## DAY 3: Thursday (Week 1)\n",
    "\n",
    "#### Setup for Repo and Documentation Push\n",
    "*Setup and testing I did to make sure my repo and documentation were ready to push at the end of the day*\n",
    "\n",
    "#### Repo File Strategy Discussion\n",
    "*How I will present my repo files for clarity and demonstration*\n",
    "\n",
    "#### Work towards milestone 1\n",
    "Collected all possible data points needed to collect from data sources.\n",
    "\n",
    "#### Work towards milestone 2\n",
    "Created list of tables needed to create in PostgreSQL db.\n",
    "\n",
    "#### Work towards milestone 3\n",
    "Perform data operations to combine data from both sources and insert unique values into new table.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import urllib.request, json\n",
    "\n",
    "# Store api key value into variable\n",
    "APIKEY_VALUE = \"c6ba6fa8-84c5-40d3-a30e-8d6d414a21c7\"\n",
    "\n",
    "# concat api query string with api key\n",
    "APIKEY = \"?hapikey=\" + APIKEY_VALUE\n",
    "\n",
    "# hs api end point stored to a variable\n",
    "HS_API_URL = \"http://api.hubapi.com\"\n",
    "\n",
    "thin_contact_list = []\n",
    "\n",
    "def get_contacts():\n",
    "    # builds the correct url\n",
    "    xurl = \"/contacts/v1/lists/all/contacts/all\"\n",
    "    url = HS_API_URL + xurl + APIKEY \n",
    "    # Now we use urllib to open the url and read it\n",
    "    response = urllib.request.urlopen(url).read()\n",
    "    # print(\"response\", response)\n",
    "    # loads to json obj to all_contacts variable\n",
    "    all_contacts = json.loads(response)\n",
    "    # return the contact data\n",
    "    return all_contacts\n",
    "\n",
    "def process_contacts(contact_list):\n",
    "    new_contact_list = []\n",
    "    \n",
    "    #create a loop through contacts dict and store values to new list\n",
    "    for i in range(len(contacts['contacts'])):\n",
    "        \n",
    "        first_name = ''\n",
    "        last_name = ''\n",
    "        for property in contacts['contacts'][i]['properties']:\n",
    "                \n",
    "            if property == 'firstname':\n",
    "                first_name = contacts['contacts'][i]['properties']['firstname']['value']\n",
    "            \n",
    "            if property == 'lastname':\n",
    "                last_name = contacts['contacts'][i]['properties']['lastname']['value']\n",
    "        \n",
    "        email = ''\n",
    "        for identity in contacts['contacts'][i]['identity-profiles'][0]['identities']:\n",
    "            if identity['type'] == 'EMAIL':\n",
    "                email = identity['value']\n",
    "        \n",
    "        created_on= contacts['contacts'][i]['addedAt']\n",
    "        last_updated = contacts['contacts'][i]['identity-profiles'][0]['saved-at-timestamp']\n",
    "\n",
    "        #created contact dict to go into db\n",
    "        contact = {\"firstname\": first_name,\n",
    "                   \"lastname\": last_name,\n",
    "                   \"email\": email,\n",
    "                   \"createdon\": created_on,\n",
    "                   \"lastupdated\": last_updated\n",
    "                  }\n",
    "\n",
    "        new_contact_list.append(contact)\n",
    "    \n",
    "    return new_contact_list\n",
    "        \n",
    "contacts = get_contacts()\n",
    "#process list of contacts\n",
    "thin_contact_list = process_contacts(contacts)\n",
    "    \n",
    "pprint(thin_contact_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## DAY 4: Tuesday (Week 2)\n",
    "\n",
    "#### Work in Progress Feedback \n",
    "*Feedback and ideas from my work in progress presentation *\n",
    "\n",
    "#### Work towards milestone 1\n",
    "Update script to collect only data I want to insert into PostgreSQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import urllib.request, json\n",
    "\n",
    "# Store api key value into variable\n",
    "APIKEY_VALUE = \"c6ba6fa8-84c5-40d3-a30e-8d6d414a21c7\"\n",
    "\n",
    "# concat api query string with api key\n",
    "APIKEY = \"?hapikey=\" + APIKEY_VALUE\n",
    "\n",
    "# hs api end point stored to a variable\n",
    "HS_API_URL = \"http://api.hubapi.com\"\n",
    "\n",
    "thin_contact_list = []\n",
    "\n",
    "def get_vids():\n",
    "    vid_list = []\n",
    "    \n",
    "    # builds the correct url\n",
    "    xurl = \"/contacts/v1/lists/all/contacts/all\"\n",
    "    url = HS_API_URL + xurl + APIKEY \n",
    "    # Now we use urllib to open the url and read it\n",
    "    response = urllib.request.urlopen(url).read()\n",
    "    # print(\"response\", response)\n",
    "    # loads to json obj to all_contacts variable\n",
    "    all_contacts = json.loads(response)\n",
    "    for i in range(len(all_contacts['contacts'])):\n",
    "        vid_list.append(all_contacts['contacts'][i]['vid'])\n",
    "        \n",
    "    return vid_list\n",
    "\n",
    "def get_urls(list):\n",
    "    url_list = []\n",
    "    for i in list:\n",
    "        xurl = \"/contacts/v1/contact/vid/:\" + str(i) + \"/profile\"\n",
    "        url = HS_API_URL + xurl + APIKEY \n",
    "        # print(url)\n",
    "        url_list.append(url)\n",
    "        #response = urllib.request.urlopen(url).read()\n",
    "        #all_properties = json.loads(response)\n",
    "    \n",
    "    return url_list\n",
    "        \n",
    "def get_properties(urls):\n",
    "    contact_list = []\n",
    "    for i in urls:\n",
    "        response = urllib.request.urlopen(i).read()\n",
    "        all_properties = json.loads(response)\n",
    "        contact_list.append(all_properties)\n",
    "    return contact_list\n",
    "        \n",
    "new_vid_list = get_vids()\n",
    "all_urls = get_urls(new_vid_list)\n",
    "#pprint(new_vid_list)\n",
    "#pprint(get_properties(all_urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Work towards milestone 2\n",
    "Update script to insert data into PostgreSQl database.\n",
    "\n",
    "- Created script for googlesheet data to be imported to PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table data created successfully\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'devleague_applicants_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-41ec931f5800>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Table data created successfully\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'devleague_applicants_data.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ISO-8859-1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'devleague_applicants_data.csv'"
     ]
    }
   ],
   "source": [
    "import psycopg2, csv\n",
    " \n",
    "con = None\n",
    "\n",
    "# try to connect\n",
    "try:\n",
    "    # adapter to connect to postgres db \n",
    "    con = psycopg2.connect(database='hsbd', user='nat') \n",
    "    # allows python code to execute sql commands\n",
    "    cur = con.cursor()\n",
    "    # execute method that process sql commands in db\n",
    "    cur.execute('''DROP TABLE IF EXISTS all_contacts_g''')\n",
    "\n",
    "    cur.execute('''CREATE TABLE all_contacts_g\n",
    "        (   date text,\n",
    "            status text,\n",
    "            stage text,\n",
    "            referal text,\n",
    "            projected_start_date text,\n",
    "            first text,\n",
    "            last text,\n",
    "            gender text,\n",
    "            term text,\n",
    "            location text,\n",
    "            skype text,\n",
    "            start_date text,\n",
    "            cohort text,\n",
    "            experience text,\n",
    "            profession text,\n",
    "            motivation text,\n",
    "            media text,\n",
    "            media2 text,\n",
    "            elevate_scholarship text,\n",
    "            notes_log text\n",
    "        );''')\n",
    "    print (\"Table data created successfully\")\n",
    " \n",
    "    reader = csv.reader(open('devleague_applicants_data.csv', 'r', encoding = 'ISO-8859-1'))\n",
    " \n",
    "    for i, row in enumerate(reader):\n",
    "        if i < 1 : continue\n",
    "        print(i, row)\n",
    "        cur.execute('''\n",
    "            INSERT INTO \"all_applicants_g\"(\n",
    "            \"date\",\n",
    "            \"status\",\n",
    "            \"stage\",\n",
    "            \"referal\",\n",
    "            \"projected_start_date\",\n",
    "            \"first\",\n",
    "            \"last\",\n",
    "            \"gender\",\n",
    "            \"term\",\n",
    "            \"location\",\n",
    "            \"skype\",\n",
    "            \"start_date\",\n",
    "            \"cohort\",\n",
    "            \"experience\",\n",
    "            \"profession\",\n",
    "            \"motivation\",\n",
    "            \"media\",\n",
    "            \"media2\",\n",
    "            \"elevate_scholarship\",\n",
    "            \"notes_log\"\n",
    "            ) values %s ''', [tuple(row)]\n",
    "        )\n",
    "    con.commit()\n",
    "\n",
    "finally:\n",
    "    \n",
    "    if con:\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Work towards milestone 3\n",
    "Push up both project documentation and portfolio repo documentation.\n",
    "- Executed data operation queries to insert data. \n",
    "- Pushed up repo docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` sql\n",
    "drop table if exists person;\n",
    "\n",
    "create table person (\n",
    "\tid serial PRIMARY KEY,\n",
    "\tfirst_name text not null,\n",
    "\tlast_name text not null,\n",
    "\tcreate_date timestamp default now(),\n",
    "\tlast_updated timestamp default now()\n",
    ");\n",
    "\n",
    "insert into person (first_name, last_name) select distinct first_name, last_name from all_contacts_hb;\n",
    "\n",
    "insert into person (first_name, last_name) select distinct first, last from all_applicants_g;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## DAY 5: Wednesday (Week 2)\n",
    "\n",
    "### Project Highlights: The things I am most excited about in my project\n",
    "Me\n",
    "- Excited to merge dump data into one table and see how to filter through duplicates with just a query.\n",
    "- Using Postico vs the command line.\n",
    "\n",
    "Peer Identified\n",
    "\n",
    "- How to use Postico\n",
    "- How to get unique values with unique ids to auto generate\n",
    "\n",
    "### Peer Repo Feedback \n",
    "\n",
    "*Here are the changes I am making to my repo structure for additional clarity* \n",
    "- Didn't use R\n",
    "- Only used Postico\n",
    "- Found out that I need to cleanse more of the data used\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 6: Thursday (Week 2)\n",
    "--- \n",
    "\n",
    "#### Things I didn't get to\n",
    "*Here are some ideas that I didn't get to implement, but wanted to. I will be adding these to my roadmap table entry for this sprint as well.\n",
    "- Getting all data from HubSpot API, still need to work on that.\n",
    "- Didn't get to use R would like to do more with that.\n",
    "- Need to finalize data model.\n",
    "- Need complete cleansed data to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
