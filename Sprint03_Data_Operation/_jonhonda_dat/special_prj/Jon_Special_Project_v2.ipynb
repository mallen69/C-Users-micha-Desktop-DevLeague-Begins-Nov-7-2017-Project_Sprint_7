{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "'''\n",
    "OBJECTIVES:\n",
    "1. Build WRS system\n",
    "2. Build Structural BMP Solution evaluator\n",
    "3. Identify minimum BMP solution front for:\n",
    "   individual facilities\n",
    "   facilities w/in departments\n",
    "   facilities w/in city\n",
    "   \n",
    "PYTHON VERSION: 3.6.3  \n",
    "SQLALCHEMY VERSION: 1.1.13\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clearing old DB\n"
     ]
    }
   ],
   "source": [
    "import winsound\n",
    "import pandas as pd\n",
    "'''\n",
    "Define basic SQLAlchemy items:\n",
    "    declarative base object\n",
    "    connection object\n",
    "    session object\n",
    "    DB tables\n",
    "'''\n",
    "#SQLAlchemy library items:\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import Column, Integer, String\n",
    "from sqlalchemy import update, insert\n",
    "from sqlalchemy import and_ #used in query.filter() to joing multiple where clauses\n",
    "from sqlalchemy import ForeignKey\n",
    "from sqlalchemy.orm import relationship #http://docs.sqlalchemy.org/en/latest/orm/basic_relationships.html#relationship-patterns\n",
    "from sqlalchemy import inspect\n",
    "\n",
    "from SQLA_Base import Base #module containing declarative_base\n",
    "from SQLA_conn_man import session, engine #module handling db and connection creation \n",
    "\n",
    "#Table definitions as SQLA classes:\n",
    "from SQLA_DB_base_bmp_feasibility_test_results import Base_BMP_Feasibility_Test_Results as BBFTR\n",
    "from SQLA_DB_base_bmp_feasibility_test_definitions import Base_BMP_Feasibility_Test_Definitions as BBFTD\n",
    "from SQLA_DB_base_bmps import Base_BMPs\n",
    "from SQLA_DB_combo_bmps import Combo_BMPs\n",
    "from SQLA_DB_combo_bmp_feasibility_test_results import Combo_BMP_Feasibility_Test_Results as CBFTR\n",
    "from SQLA_DB_expressions import Expressions\n",
    "from SQLA_DB_facility_chars import Facility_Chars\n",
    "from SQLA_DB_facility_monthly_rain import Facility_Monthly_Rain\n",
    "from SQLA_DB_facility_risks import Facility_Risks\n",
    "from SQLA_DB_facility_type_has_nel import Facility_Type_Has_NEL\n",
    "from SQLA_DB_facility_types import Facility_Types\n",
    "from SQLA_DB_feasibility_test_questions import Feasibility_Test_Questions as FTQ\n",
    "from SQLA_DB_nel_sample_classes import NEL_Sample_Classes\n",
    "from SQLA_DB_existing_pollutant_concentrations import Existing_Pollutant_Concentrations as ExPollConcs\n",
    "from SQLA_DB_pollutant_removal_rates import Pollutant_Removal_Rates as PRR\n",
    "from SQLA_DB_wrs_pollutant_risks import WRS_Pollutant_Risks\n",
    "Base.metadata.create_all(engine, checkfirst=True) #create SQLA classes\n",
    "\n",
    "'''\n",
    "Dictionary of \"SQLAlchemy where clause lambda functions\" that importCSV uses to test record uniqueness.\n",
    "used as the where clause in sqlalchemy queries, updates and deletes \n",
    "Form:\n",
    "    {TableName:Lambda Function, TableName:Lambda Function, ...}\n",
    "    \n",
    "    TableName is the table name we want to define uniqueness test for\n",
    "    Lambda Function is a SQLAlchemy query used to test record uniqueness. The function can take on any form \n",
    "        but must be made to evaluate the CSV row passed as a dictionary (CSVRowDict in this explanation):\n",
    "        CSVRowDict: {FieldName:CSVColValue, DBTableFieldName:CSVColValue...} \n",
    "            Where: DBTableFieldName is the name of the field associated with the value at CSVColValue on the current row\n",
    "                   CSVColValue: a value in the CSV's current row+column corresponding to the DBTableFieldName \n",
    "        *this assumes that field names are unique across table. if not, then method fails (maybe need to extend method?)\n",
    "    FALSE: indicates that db table doesn't impose uniqueness on its records (other than its record id being unique)\n",
    "        \n",
    "e.g.: lambda myRowVal: Base.metadata.tables['people'].c['name'] == CSVRowDict['name']\n",
    "        using lambda function in query will search for CSVRowDict's value for 'name' in the table people, field name \n",
    "if table has no record uniqueness requirement, then enter: TableName:False\n",
    "'''\n",
    "unqTests = {\n",
    "    'facility_chars': lambda CSVRowDict: Base.metadata.tables['facility_chars'].c['Fac_Name'] == CSVRowDict['Fac_Name'],\n",
    "    'facility_monthly_rain': False, #DB schema does not impose uniqueness on records in this table\n",
    "    'facility_type_has_nel': False,\n",
    "    'facility_risks': False,\n",
    "    'facility_types': lambda CSVRowDict: Base.metadata.tables['facility_types'].c['Fac_Type'] == CSVRowDict['Fac_Type'],\n",
    "    'nel_sample_classes': lambda CSVRowDict: Base.metadata.tables['nel_sample_classes'].c['nel_column']==CSVRowDict['nel_column'],\n",
    "    'existing_pollutant_concentrations': False, #uniqueness not imposed for records in this table.\n",
    "    'wrs_pollutant_risks': False #DB schema does not impose uniqueness on records in this table\n",
    "}\n",
    "\n",
    "import SQLA_main as SQLA_main #import main SQLAlchemy functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Define other custom modules\n",
    "\n",
    "'''\n",
    "import mod_Base_BMP_Eval as BBMP_Eval\n",
    "import mod_Combo_BMP_Eval as CBMP_Eval\n",
    "import mod_EffluentLimit as EffLim\n",
    "import mod_expression as Expr\n",
    "import mod_importSpecial as importSpecial #special import functions are defined here\n",
    "import mod_importCSV as importCSV #generic CSV importer ****IMPORTANT NOTE: function assumes csv in the utf-8-sig file format. weird things happen if its not in this format!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import feasibillity questions, build feasibility expressions\n",
    "importSpecial.importFeasibilityQuestionsCSV('Input_Files\\\\feasibility_test_questions.csv') \n",
    "\n",
    "#import base bmp information including:\n",
    "  #1. imports definitions for cip costs, o&m costs, and BMP sizing to the expressions table\n",
    "  #2. imports pollutant removal rates into pollutant_removal_rates table\n",
    "  #3. creates a record in the base_bmps table using (1) and (2)\n",
    "  #4. feasibility tests\n",
    "importSpecial.importBaseBMPsCSV('Input_Files\\\\bmp_lego_piece.csv') \n",
    "\n",
    "#IMPORT BASIC FACILITY CHARS:\n",
    "    #!!!!IMPORTANT!!!! This import must occur before other facility specific data is imported!\n",
    "print ('\\nImporting facility characteristics:')\n",
    "importCSV.importCSV('Input_Files\\\\facility_chars.csv', unqTests)\n",
    "\n",
    "#IMPORT PBP Appendix A1 data\n",
    "print ('\\nImporting PBP Appendix A1 data:')\n",
    "importCSV.importCSV('Input_Files\\\\pbp_appxa1.csv', unqTests)\n",
    "\n",
    "#IMPORT FACILITY RAINFALL EXTRACTED FROM http://rainfall.geography.hawaii.edu/downloads.html\n",
    "print ('\\nImporting Facility Rainfall Data:')\n",
    "importCSV.importCSV('Input_Files\\\\FacilityRainfallData.csv', unqTests)\n",
    "\n",
    "#IMPORT EFFLUENT LIMITS EXISTANCE FOR FACILITY TYPES: (either by Priority Based Plan, Table 3 or as City operational assignment)\n",
    "#IF CSV HEADRS SETUP CORRECTLY, THEN THIS INSERTS NEL EXISTANCE DATA (0 OR 1) TO WRS_POLLUTANT TABLE \n",
    "#AND USES THE FACILITY_TYPE_HAS_NEL TO ASSOCIATE RECORD WITH FACILITY TYPE\n",
    "print ('\\nImporting Facility Type Has Effluent Limits:') #import into wrs_pollutant_risks table\n",
    "importCSV.importCSV('Input_Files\\\\nel_exists_facility_types.csv', unqTests)\n",
    "\n",
    "#IMPORT NEL CLASSIFICATION DATA (from PBP Appendix L)\n",
    "print ('\\nImporting NEL Classes')\n",
    "importCSV.importCSV('Input_Files\\\\nel_pbp_appxl.csv', unqTests)\n",
    "\n",
    "#IMPORT FACILITY RISKS:\n",
    "print ('\\nImporting Facility Risks')\n",
    "#for future implementation:\n",
    "    #The current process inserts fac risk and update existing_fac_char_id in Facility_chars table. this process thus creates\n",
    "#dead records. a more sophisticated approach using sophisticated lambda function in unqTests would fix this\n",
    "importCSV.importCSV('Input_Files\\\\facility_risks.csv', unqTests)\n",
    "\n",
    "# #IMPORT FACILITY SAMPLING DATA\n",
    " #!!!IMPORTANT!!!! For now, we make none detects = 0 BUT this must be changed to detection limit, per DOH guidance.\n",
    "print ('\\nImporting Facilty Sampling data:')\n",
    "importCSV.importCSV('Input_Files\\\\sample_data.csv', unqTests)\n",
    "\n",
    "\n",
    "# for now, since we're developing, delete out all except 1st 2 facilities.\n",
    "# n = 5\n",
    "# session.query(ExPollConcs).filter(ExPollConcs.facility_id >n).delete(synchronize_session = False) #http://docs.sqlalchemy.org/en/latest/orm/query.html#sqlalchemy.orm.query.Query.delete\n",
    "# session.query(Facility_Chars).filter(Facility_Chars.id >n).delete(synchronize_session = False) #http://docs.sqlalchemy.org/en/latest/orm/query.html#sqlalchemy.orm.query.Query.delete\n",
    "# session.commit #we chose not to sync session so need to commit before proceeding to requery or else you may get unpredictable resutls\n",
    "\n",
    "session.commit()\n",
    "winsound.Beep(250,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # EVALUATE BASE BMP FEASIBILITY at each facility\n",
    "# # Write results to the base_bmp_feasibility_test_results table.\n",
    "\n",
    "# print('\\n******Evaluating Base BMP feasibility at facilities.******')\n",
    "# ShowCalculations = False #flag indicating if steps should be outputted\n",
    "# Expr.ResetEvalErrorCount() #RESET EXPRESION EVALUATOR ERROR COUNT\n",
    "# for aFac in session.query(Facility_Chars):\n",
    "#     if ShowCalculations: print ('\\n***Evaluating base bmp feasibiilty tests for facility: ', aFac.Fac_Name), ' ***'\n",
    "#     myBMPs = session.query(Base_BMPs)\n",
    "#     for aBMP in myBMPs:\n",
    "#         if ShowCalculations:print ('\\nEvaluating feasibility of base_bmp: ', aBMP.bmp_name, ' ID: ', aBMP.id)\n",
    "#         BBMP_Eval.Eval_base_bmp_feasibility_tests(aFac, aBMP, False)\n",
    "# session.commit\n",
    "# winsound.Beep(250,1000)\n",
    "# print ('*****************************************************************')\n",
    "# print ('* Completed evaluating Base BMP feasibility                     *')\n",
    "# if Expr.CountEvalErrors() >0:\n",
    "#     print (Expr.CountEvalErrors(), ' errors were encountered. Review output to identify location(s)')\n",
    "#     print ('Hint: expression evaluation error lines are prefixed by: FAULT!!!! Error occured while evaluating expression:')\n",
    "# else:\n",
    "#     print ('No errors detected.')\n",
    "# print ('*****************************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Enter estimated pollutant concentrations into database's existing pollutant concentration table for facilities without \n",
    "actual sampling data. Use 1 of 2 methods:\n",
    "\n",
    "Method 1: Use maximum concentration value sampled for period 2013-2017\n",
    "          This method is for Permit Table 1 facilities only\n",
    "          Method assumes we have already entered sampling data for into the database's existing pollutant concentration table\n",
    "\n",
    "Method 2: Use data from an EMC study.\n",
    "          This method is for facilities that are not on Permit Table 1\n",
    "          \n",
    "          \n",
    "Defined Global Vars:\n",
    "    pd_infieldExtrema: dataframe holding maximum concentrations that were measured. *exception is phMin. we use lowest observed ph\n",
    "    \n",
    "    \n",
    "'''\n",
    "pollLS = ['tss', 'turbidity', 'p', 'n', 'nn', 'an', 'og', 'cu', 'zn', 'fe', 'phmin', 'phmax'] #list of pollutant constituants we're trying to address\n",
    "\n",
    "\n",
    "#get all existing pollutant concentrations obtained using the 'infield' sampling method\n",
    "q = session.query(ExPollConcs).filter(ExPollConcs.sample_method == 'infield')\n",
    "pd_Concs = pd.read_sql(q.statement,session.bind) \n",
    "\n",
    "#build pd_infieldExtreama by making a dictionary of maximum sample results for each constiuent\n",
    "dict_extrema = {'c_' + Constituent: pd_Concs.loc[:,'c_' + Constituent].max() for Constituent in pollLS}\n",
    "dict_extrema['c_phmin'] = pd_Concs.loc[:,'c_phmin'].min() #phMin is exception to above. we want min. phMin value\n",
    "#use dictionary to build pd_infieldExtrema dataframe\n",
    "pd_infieldExtrema = pd.DataFrame([dict_extrema])\n",
    "display(pd_infieldExtrema)\n",
    "\n",
    "#Apply Method1 to all unsampled Table 1 facilities\n",
    "#we need to get all Table 1 facilities that are not currently in the existing pollutant concentration table\n",
    "#############################################################################################################\n",
    "'''\n",
    "To be sure we're starting fresh, let's remove any records in ExPollConcs that:\n",
    "1. Were not obtained directly from field samples (i.e. sample_method != 'infield)\n",
    "2. Were obtained from field samples, but are not Table 1 facilities (i.e. we shouldn't be looking at their  sample results)\n",
    "'''\n",
    "#delete all pollutant concentration table records that are not from infield sampling.\n",
    "session.query(ExPollConcs).filter(ExPollConcs.sample_method != 'infield').delete(synchronize_session = False)\n",
    "#delete all pollutant concentration table records that are not for Table 1 facilities\n",
    "#for some reason bulk delete's not working. so let's use a loop to work around it.\n",
    "for rec in session.query(ExPollConcs.id).filter(ExPollConcs.facility_id == Facility_Chars.id).filter(Facility_Chars.Permit_Table != 'Table 1'):\n",
    "    session.query(ExPollConcs).filter(ExPollConcs.id == rec[0]).delete(synchronize_session = False)\n",
    "\n",
    "\n",
    "#now build query that identifies all Table 1 facilities that are not in ExPollConcs\n",
    "subq = session.query(ExPollConcs.facility_id.distinct()).order_by(ExPollConcs.facility_id).all()\n",
    "ls_sq = [i[0] for i in subq if i[0] is not None] #list comprehension to produce list of all facility_id in ExPollConcs table\n",
    "#get list of Table 1 facilities not in ExPollConcs:\n",
    "tpl_q = session.query(Facility_Chars.id).filter(Facility_Chars.Permit_Table == 'Table 1').filter(Facility_Chars.id.notin_(ls_sq)).all()\n",
    "ls_FacIDs = [i[0] for i in tpl_q] #write query tuple to list    \n",
    "#make a list of Table 1 facs not in ExPollConcs (a list of dicts). also include extrema conc. values.  \n",
    "ls_dict_pd = [{**{'facility_id': FacID, 'sample_method': 'sim_MaxType', 'sample_date':'12/31/2016'}, **dict_extrema} for FacID in ls_FacIDs]\n",
    "#write list to database:\n",
    "ExPollConcs_meta = Base.metadata.tables['existing_pollutant_concentrations']\n",
    "ExPollConcs_id_meta = ExPollConcs_meta.c['id']\n",
    "for dict_temp in ls_dict_pd:\n",
    "    SQLA_main.insertRec(ExPollConcs_meta,dict_temp)\n",
    "session.commit()\n",
    "#for future implementation: write dict -> dataframe -> db(using sqla):\n",
    "    # pd_temp.to_sql('existing_pollutant_concentrations', engine, if_exists='append', index = False)\n",
    "    #http://docs.sqlalchemy.org/en/latest/faq/performance.html#i-m-inserting-400-000-rows-with-the-orm-and-it-s-really-slow\n",
    "    #https://stackoverflow.com/questions/31997859/bulk-insert-a-pandas-dataframe-using-sqlalchemy\n",
    "q = session.query(ExPollConcs)\n",
    "pd.read_sql(q.statement,session.bind) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Estimate Pollutant Effluent Limits\n",
    "'''\n",
    "Estimate the Numeric Effluent Limits (NELs) for each facility.\n",
    "Return wet and dry season NELs in 2 separate dataframes:\n",
    "    pd_FacsNELs_Wet & pd_FacsNELs_Dry\n",
    "Estimate NELs using the EffLim module's GetNELs function call.\n",
    " The GetNELs function call will differentiate between wet and dry season limits\n",
    " (if limits are the same between wet & dry season, then the same limit will be placed into the wet and dry\n",
    "  dataframes.)\n",
    " The GetNEls function calculates a pollutant constituent NEL using this formula:\n",
    "    NEL = fTypeHas_NEL * SampleClass_NEL\n",
    "    Where:\n",
    "      fTypeHas_NEL is a [0,1] value from PBP Table 3, based on facility type (stored in SQLA_DB_facility_type_has_nel)\n",
    "      SampleClass_NEL is pollutant concentration based on facility's sample class, based on PBP Appendix L\n",
    "'''\n",
    "pd_FacsNELs_Wet, pd_FacsNELs_Dry = pd.DataFrame(),  pd.DataFrame() #initialize wet and dry season nel dataframes \n",
    "for recFac in session.query(Facility_Chars): #do the following for each facility:\n",
    "    wet,dry = EffLim.GetNELs(recFac,False) #Get Wed & Dry NELs by calculating: NEL = fTypeHas_NEL * SampleClass_NEL\n",
    "#     if wet is not None:\n",
    "    pd_FacsNELs_Wet = pd.concat([pd_FacsNELs_Wet, wet]) #write wet NELs to pd_FacsNELs_Wet\n",
    "#     if dry is not None:\n",
    "    pd_FacsNELs_Dry = pd.concat([pd_FacsNELs_Dry, dry]) #write dry NELs to pd_FacsNELs_Dry\n",
    "\n",
    "print('Wet NELs:')\n",
    "display(pd_FacsNELs_Wet)\n",
    "print('Dry NELs:')\n",
    "display(pd_FacsNELs_Dry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#let's work on calculating exceedance values\n",
    "\n",
    "import mod_EffluentLimit as EffLim\n",
    "\n",
    "#get all sampling data\n",
    "q = session.query(ExPollConcs.id, ExPollConcs.facility_id.label('Facility_ID'), ExPollConcs.sample_date, \n",
    "        ExPollConcs.c_tss,\n",
    "        ExPollConcs.c_turbidity,\n",
    "        ExPollConcs.c_p,\n",
    "        ExPollConcs.c_n,\n",
    "        ExPollConcs.c_nn,\n",
    "        ExPollConcs.c_an,\n",
    "        ExPollConcs.c_og,\n",
    "        ExPollConcs.c_cu,\n",
    "        ExPollConcs.c_zn,\n",
    "        ExPollConcs.c_fe,\n",
    "        ExPollConcs.c_phmin,\n",
    "        ExPollConcs.c_phmax  \n",
    "         ).order_by(ExPollConcs.facility_id) #.filter(ExPollConcs.facility_id == FacID)\n",
    "pd_Concs = pd.read_sql(q.statement,session.bind) \n",
    "#tidy up the sampling data\n",
    "from datetime import datetime\n",
    "pd_Concs['sample_date'] = pd.to_datetime(pd_Concs['sample_date'], format=\"%m/%d/%Y\")\n",
    "pd_Concs = pd_Concs.applymap(lambda x: float('nan') if x is None else x) #assign NaN values to any None element\n",
    "\n",
    "#for each pollutant constituent, do the Exceedance Calculation = max(0,(Constituent Concentration - NEL))\n",
    "# if no exceedance, then report 0. report NaN sample result is NaN\n",
    "pd_FacExceedances = pd.DataFrame() #make an empty dataframe.  we will append to it.\n",
    "for Facs in session.query(Facility_Chars.id).order_by(Facility_Chars.id):\n",
    "    FacID = Facs[0]\n",
    "    if (FacID in pd_Concs.Facility_ID.values) and (FacID in pd_FacsNELs_Dry.index) and (FacID in pd_FacsNELs_Wet.index):\n",
    "        pd_temp = pd_Concs.loc[pd_Concs['Facility_ID'] == FacID] #slice facility id rows into a temp dataframe\n",
    "        pd_temp.is_copy = False #acknowledge that pd_temp is NOT a copy of pd_Concs. but intended to be treated as new dataframe\n",
    "        for Constituent in pollLS:\n",
    "            pd_temp['c_' + Constituent]  = pd_Concs.apply(lambda row: #for each row in pd_Concs:\n",
    "                                     EffLim.ExceedanceCalc(row, Constituent, FacID, pd_FacsNELs_Wet, pd_FacsNELs_Dry, True), axis = 1)\n",
    "#         display(pd_temp)\n",
    "        pd_FacExceedances = pd.concat([pd_FacExceedances,pd_temp])\n",
    "print('Concentrations in excess of wet/dry season NELs')\n",
    "display(pd_FacExceedances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "CALCULATE AGE FACTOR WEIGHTED AVERAGE FOR EACH CONSTITUENT:\n",
    "\n",
    "Age factor acknowledges fact that more recent samples are a better representation of facility pollutant discharge \n",
    "(i.e. sampling data) and housekeeping-operations (i.e. inspections) realities. But, historic data as a whole also tells part \n",
    "of story (i.e. we want to dampen whipsaw effects that may occur if we only considered most recent data).\n",
    "\n",
    "AF = exp(-SampleRank)\n",
    "SampleRank = Newest sample = 1\n",
    "              Second Newest sample = 2\n",
    "              ...\n",
    "              nth Newest Sample = n (out of n samples)\n",
    "'''\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "#calculate age factor weighted averages for each constituent of each facility.\n",
    "#write these averages into a dataframe called pd_AFWFacExceedances\n",
    "#use exceedance values in pd_FacExceedances\n",
    "#we will process each facility separately. The results will be added to the pd_AFWFacExceedances dataframe\n",
    "pd_AFWFacExceedances = pd.DataFrame() #make an empty dataframe.  we will append to it.\n",
    "ShowCalculations = False\n",
    "UnqFacIDs = pd_FacExceedances.Facility_ID.unique() #get unique facility IDs\n",
    "for FacID in UnqFacIDs:\n",
    "    pd_temp = pd_FacExceedances.loc[pd_FacExceedances['Facility_ID'] == FacID].set_index('sample_date') #write current fac's data to temp df\n",
    "    pd_AFWFacExceedances = pd.concat([pd_AFWFacExceedances,EffLim.Calc_AFWtdExceedances(pd_temp,pollLS,ShowCalculations)]) #calculate age factor weighted averages\n",
    "print ('Age Factor Weighted Averages:')\n",
    "display(pd_AFWFacExceedances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#CREATE COMBO BMPS USING BASE BMPS\n",
    "#ALL POSSIBLE COMBOS WILL BE CREATED AND ADDED TO THE COMBO_BMPS TABLE\n",
    "#MAXIMUM POLLUTANT REMOVAL RATES ARE DETERMINED BY IDENTIFYING \n",
    "#  THE BASE_BMP IN THE COMBO THAT PROVIDES THE HIGHEST REMOVAL RATE FOR A GIVEN POLLUTANT\n",
    "\n",
    "import time\n",
    "print ('get a coffee...this one takes a while!')\n",
    "start_time = time.time()\n",
    "CBMP_Eval.Make_ALL_bmp_base_option_combos()\n",
    "session.commit()\n",
    "print ('--- %s execution time in seconds ---' % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Identify the feasible bmp combinations for each facility\n",
    "Use base bmp feasibility results for each facility.\n",
    "Put results into the combo_bmp_feasibility_test_results table\n",
    "'''\n",
    "import itertools     #https://docs.python.org/3/library/itertools.html    \n",
    "import pandas as pd\n",
    "\n",
    "from sqlalchemy import and_\n",
    "\n",
    "def _Make_bmp_fingerprint(base_BMP_components):\n",
    "    #create fingerprint of the passed list of base_bmp_ids\n",
    "    #fingerprint is just a | separated list of ids of the base bmps that make up the combo bmp\n",
    "    #corresponds to bmp_options table's bmp_fingerprint field\n",
    "    #FORMAT: |bmp_option_base_component_id||bmp_option_base_component_id| w/ id's given in ascending order\n",
    "    fingerprint = '|' + '|'.join(str(id) + '|' for id in base_BMP_components)\n",
    "    return fingerprint\n",
    "\n",
    "def Eval_FacBMPCombo(pd_basebmps, myFacility, bmpCombo):\n",
    "    '''\n",
    "    input:\n",
    "        pdbasebmps: pandas built from a BBMP_Eval.evalFacility_BaseBMP dictionary list\n",
    "                    assme that pandas is passed in w/ index is set as base_bmp_id\n",
    "        myFacility: SQLA fac_chars record\n",
    "        bmpCombo: list of base_bmp_ids that make up this combo\n",
    "    \n",
    "    #retrieve previously computed combo removal rate\n",
    "    #calculate combo cip and om cost, insert/update database\n",
    "    #calculate wrs reduction, insert/update database\n",
    "\n",
    "    #return as pandas    \n",
    "    '''    \n",
    "    #get combo bmp pollutant removal rates into pandas \n",
    "    q = session.query(Combo_BMPs.bmp_fingerprint, Combo_BMPs.id.label('combos_bmp_id'), PRR.id.label('PRR_id'),\n",
    "          PRR.r_tss, PRR.r_turbidity, PRR.r_p, PRR.r_n, PRR.r_nn, PRR.r_an,\n",
    "          PRR.r_og, PRR.r_cu, PRR.r_zn, PRR.r_fe, PRR.r_phmin, PRR.r_phmax\n",
    "        ).filter(Combo_BMPs.bmp_fingerprint == _Make_bmp_fingerprint(bmpCombo)).filter(\n",
    "        Combo_BMPs.bmp_option_removal_rate_id == PRR.id)  \n",
    "    pd_rr = pd.read_sql(q.statement,session.bind) \n",
    "\n",
    "    #use information in pd_rr to get CBFTR_record - make new record if necessary\n",
    "    myCBFTR = Base.metadata.tables['combo_bmp_feasibility_test_results']\n",
    "    myCBFTR_id = SQLA_main.insertupdateRec(myCBFTR,{'facility_id':myFacility.id, 'combo_bmps_id':pd_rr['combos_bmp_id'][0]},\n",
    "                and_(\n",
    "        myCBFTR.c['facility_id'] == myFacility.id,\n",
    "        myCBFTR.c['combo_bmps_id'] == pd_rr['combos_bmp_id'][0]\n",
    "                    ))\n",
    "    session.flush()\n",
    "    \n",
    "    print (myCBFTR_id)\n",
    "    \n",
    "    #calculate WRS reduction\n",
    "#     myFac_exWRSData = session.query(\n",
    "    \n",
    "    \n",
    "\n",
    "    #get costs in pandas\n",
    "    sumCIP = sum(pd_basebmps.loc[bmp_id,'calc_cip_cost'] for bmp_id in bmpCombo)\n",
    "    sumOM = sum(pd_basebmps.loc[bmp_id,'calc_om_cost'] for bmp_id in bmpCombo)\n",
    "    pd_sums = pd.DataFrame([{'calc_cip_cost':sumCIP, 'calc_om_cost': sumOM}])\n",
    "\n",
    "    #merge combo bmp's removal rates and costs into 1 dataframe\n",
    "    return pd.concat([pd_rr, pd_sums], axis = 1)\n",
    "    \n",
    "    \n",
    "\n",
    "def Eval_FacBMPOptions(myFacility):\n",
    "    #a wrapper around Eval_FacBMPCombo\n",
    "    print('\\n***Evaluating feasible bmp combos for facility: ', aFac.Fac_Name, '***')\n",
    "    print ('****Evaluating feasibile base bmps****')\n",
    "    df = pd.DataFrame(BBMP_Eval.evalFacility_BaseBMP(aFac, False)).set_index('base_bmp_id')\n",
    "    display (df)   \n",
    "    print ('****These are the feasible base bmps. I\\'ll use them to make combos:****')\n",
    "    df = df.loc[df['is_feasible'] == 1]\n",
    "    display (df)\n",
    "    feas_ls = df.index #send feasible base bmp ids to list\n",
    "#     print (feas_ls)\n",
    "# from SQLA_DB_combo_bmps import Combo_BMPs\n",
    "# from SQLA_DB_combo_bmp_feasibility_test_results import Combo_BMP_Feasibility_Test_Results as CBFTR\n",
    "    for CBOLen in range (1, len(feas_ls)+1): #+1 so it's inclusive of last count\n",
    "        for combo in  itertools.combinations(feas_ls,CBOLen):\n",
    "            print ('Here is a summary of the combo: ', list(combo))\n",
    "            display(Eval_FacBMPCombo(df,myFacility, list(combo)))\n",
    "\n",
    "            \n",
    "def Eval_All_FacBMPOptions():\n",
    "    print ('Evaluating feasibile BMP Options for each facility:')\n",
    "    for aFac in session.query(Facility_Chars):\n",
    "        Eval_FacBMPOptions(aFac)\n",
    "    \n",
    "Eval_All_FacBMPOptions()\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# session.close()\n",
    "# engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://pythonhow.com/accessing-dataframe-columns-rows-and-cells/\n",
    "import pandas as pd #import in pandas library\n",
    "print ('#get csv data and read into pandas')\n",
    "df1=pd.read_csv(\"http://pythonhow.com/wp-content/uploads/2016/01/Income_data.csv\")\n",
    "print (df1)\n",
    "print ('#write new dataframe w/ index set to the \"State\" column in the csv')\n",
    "df2=df1.set_index(\"State\").copy()\n",
    "print (df2)\n",
    "print ('#extract a portion of the dataframe: States = Alaska to Arkansas; and Dates 2005:2007')\n",
    "print (df2.loc[\"Alaska\":\"Arkansas\",\"2005\":\"2007\"])\n",
    "\n",
    "print ('Get only certain States, using a list of states:')\n",
    "getStates = ['Alaska', 'Arizona']\n",
    "print (df2.loc[getStates])\n",
    "\n",
    "print ('#slice a column:')\n",
    "df2.loc[: , \"2005\"]\n",
    "print ('get a cell:')\n",
    "df2.loc['Alaska','2005']\n",
    "print ('#get max of 2005 data')\n",
    "print (df2.loc[:,'2005'].max())\n",
    "print ('take 2005 column and put into list')\n",
    "LS = df2['2005'].tolist() #this is a series. we use the .tolist() to convert from series to list\n",
    "print (type(LS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'col1' : [1.0] * 5, \n",
    "                   'col2' : [2.0] * 5, \n",
    "                   'col3' : [3.0] * 5 }, index = range(1,6),)\n",
    "display(df)\n",
    "df2 = pd.DataFrame({'col1' : [10.0] * 5, \n",
    "                    'col2' : [100.0] * 5, \n",
    "                    'col3' : [1000.0] * 5 }, index = range(1,6),)\n",
    "display(df2)\n",
    "df.mul(df2, 0) # element by element multiplication no problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# xmin = datetime.datetime.strptime('1/1/2018', \"%m/%d/%Y\").date()\n",
    "# xmax = datetime.datetime.strptime('5/6/2018', \"%m/%d/%Y\").date()\n",
    "\n",
    "# xmin <= datetime.date(2018,1,5) <= xmax\n",
    "\n",
    "#     Wet Season is from: January 1 through April 30 and November 1 through December 31\n",
    "#     Dry Season is from: May 1 through October 31\n",
    "\n",
    "SampleDate = datetime.date(2018,11,1)\n",
    "\n",
    "#Wet Season 1:\n",
    "if datetime.date(SampleDate.year, 1,1) <= SampleDate <= datetime.date(SampleDate.year, 4,30):\n",
    "    print ('ws 1')\n",
    "elif datetime.date(SampleDate.year, 5,1) <= SampleDate <= datetime.date(SampleDate.year, 10,31):\n",
    "    print ('dry')\n",
    "else:\n",
    "    print ('ws 2')\n",
    "    \n",
    "    \n",
    "import numpy as np    \n",
    "# np.max([float('nan'),0])\n",
    "np.max([0,float('nan')])\n",
    "\n",
    "if math.isnan(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import the pandas library\n",
    "import pandas as pd\n",
    "\n",
    "ipl_data = {'Team': ['Riders', 'Riders', 'Devils', 'Devils', 'Kings',\n",
    "         'kings', 'Kings', 'Kings', 'Riders', 'Royals', 'Royals', 'Riders'],\n",
    "         'Rank': [1, 2, 2, 3, 3,4 ,1 ,1,2 , 4,1,2],\n",
    "         'Year': [2014,2015,2014,2015,2014,2015,2016,2017,2016,2014,2015,2017],\n",
    "         'Points':[876,789,863,673,741,812,756,788,694,701,804,690]}\n",
    "df = pd.DataFrame(ipl_data)\n",
    "\n",
    "display (df)\n",
    "\n",
    "display (df.groupby('Team').groups)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "grouped = df.groupby('Year')\n",
    "print (grouped['Points'].agg(np.max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'col1': [1, 2], 'col2': [0.1, 0.2]},\n",
    "                      index=['a', 'b'])\n",
    "for idx in df.index:\n",
    "    print (idx)\n",
    "    print (df.loc[idx]['col2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t1 = True\n",
    "t2 = True\n",
    "print (t1 and t2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
