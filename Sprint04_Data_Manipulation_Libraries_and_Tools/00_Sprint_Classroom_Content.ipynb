{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Manipulation Libraries and Tools\n",
    "*Module: Basic Data Manipulation (Sprint 2 of 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sprint Module Review and Data Stories\n",
    "\n",
    "#### Basic Data Manipulation\n",
    "*Before you engage in structured analysis, you often just want to see the data. This can mean pre-viewing a subset of it, summarize the columns/attributes/features, sorting or reorganizing it and otherwise finding ways to immerse yourself in your data. Different technologies tools have something different to offer, and our objective is to develop a good sense of the utilities available to you*\n",
    "\n",
    "|Data Journalist| Data Engineer | Statistical Modeler| Business Analyst |\n",
    "|----|----------------|------------------|----|\n",
    "|… I need to be able to **convert published research and analysis from Excel / R / Python** into a different tool so I can verify and audit the analysis|… I need to understand the **basic data structures in Python** so that I can diagnose and troubleshoot performance issues|…I need to understand the **NumPy arrays and Pandas / R dataframes** so I can supply data to algorithms, fit models, etc|… I need to understand how to export my **advanced excel skills to R / Python** so that I can build more powerful analyses on top of what I already know|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytical Process Big Picture\n",
    "![Curriculum Summary](../curriculum_summary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Tools Capstone\n",
    "What you have so far:\n",
    "- A map/taxonomy of technologies: Programming languages, editors, command line environments, source control, publishing, cloud computing\n",
    "- awareness of several \"languages\": SQL, R, Python\n",
    "- a basic idea of the components of programming\n",
    "- vocabulary of data and computing concepts\n",
    "- some experience assembling some of these pieces into projects, and using these tools\n",
    "\n",
    "### Tools development never stops\n",
    "You will not master these tools today. You won't master them this year. You won't master them in 10 years. You **will** develop a clearer and clearer picture of what you need to do and the most efficient ways to do it with increased **practice** and **experience**\n",
    "\n",
    "This sprint is about building a map of the available tools so you can access them when ready, and drafting a process that you can use to learn them on a continuing basis\n",
    "\n",
    "### Where do data manipulation tasks fit into the big picture above?\n",
    "- Exploratory Data Analysis:\n",
    "        - Creating\n",
    "        - Combining\n",
    "        - Converting\n",
    "        - Cleaning\n",
    "- Descriptive Statistics: Formatting / Summarizing\n",
    "- Basic Data Visualization: Formatting Data / Visualizing\n",
    "- Inferential Statistics: Sampling\n",
    "- Modeling: Preparing Data for input into\n",
    "- Data Governance: Anonymizing and Sanitizing\n",
    "        - Removing\n",
    "        - encrypting\n",
    "- Production Development: Everything\n",
    "- Data Products\n",
    "        - Creating APIs\n",
    "        - exporting data\n",
    "        - connecting data to clients\n",
    "        - formatting it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Key Concepts and Definitions\n",
    "- library\n",
    "- package\n",
    "- function\n",
    "- function call\n",
    "- ecosystem\n",
    "- data-wrangling\n",
    "- Pandas\n",
    "- Numpy\n",
    "- Tidyverse\n",
    "- array operations\n",
    "- row/column/table/element-\"wise\" operations\n",
    "\n",
    "\n",
    "\n",
    "## Key Questions\n",
    "- What is a library?\n",
    "- What is a package?\n",
    "- How do I get them?\n",
    "- How are they created?\n",
    "- What libraries / packages are available to me?\n",
    "- What do each of them do?\n",
    "- How do I learn about them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do the Libraries do?\n",
    "- https://www.quora.com/What-are-the-Python-libraries-that-are-used-by-data-scientists/answer/Jared-Stufft-1\n",
    "\n",
    "### EVERYTHING. So why do we need to do anything? \n",
    "- Swiss Army knife: Clever, but takes some practice\n",
    "- In Particular: Data Wrangling\n",
    "\n",
    "### What is Data Wrangling?\n",
    "> Unfortunately, data wrangling is 80% of what a data scientist does. It’s where most of the real value is created and it’s the most thankless, difficult, and poorly understood job I know of. Nobody gets a degree in data wrangling. Nobody publishes papers on it. Nobody teaches how to do it. (Yes there are courses on how to use specific tools like R or Python to do simple joins and dupe removal, but they assume that you already know how and why you are wrangling.)\n",
    "\n",
    "> There are six steps in data wrangling:\n",
    "\n",
    "> Gather data from inside and outside the firewall\n",
    "Understand (and document) your sources and their limitations\n",
    "Clean up the duplicates, blanks, and other simple errors\n",
    "Join all your data into a single table\n",
    "Create new data by calculating new fields and recategorizing\n",
    "Visualize the data to remove outliers and illogical results\n",
    "The first four are straightforward albeit annoying. Most people do steps 1 and 3 and then jump in to do their analysis. They then spend several weeks discovering all kinds of additional errors as they try to get their models to work.\n",
    "\n",
    "> https://www.quora.com/What-exactly-is-Data-wrangling/answer/Dan-Haight\n",
    "\n",
    "\n",
    "\n",
    "### Python Ecosystem\n",
    "\n",
    "![Python Ecosystem](00_images/python.jpeg)\n",
    "- https://www.quora.com/What-is-the-relationship-among-NumPy-SciPy-Pandas-and-Scikit-learn-and-when-should-I-use-each-one-of-them/answer/Jeremy-Langley\n",
    "- http://pandas.pydata.org/pandas-docs/stable/basics.html\n",
    "\n",
    "These are all tools in the field of data science. The lower level you get the faster speeds you can achieve. The higher level you go the more interesting problems you might be able to solve. You can see which sits on top of another thanks to Jake Vanderplas\n",
    "\n",
    "\"Numpy is the lowest level sitting on Python. It reads in fixed datatypes. It's data layout is more concerned with efficiency of memory. If you are dealing with strings they are fixed length strings. (fit the data size for each element to the longest string length) but it shines when you are dealing with number calculations. The more you can think in vectors the faster your code runs. (learn how to get rid of the for statements for speed reasons by using Numpy broadcasting)\n",
    "\n",
    "Pandas is spreadsheets for Python (something like R). It's able to describe the data for you. It can do grouping and pivot tables on larger data than most spreadsheet programs out there. The only limit (currently) is how much RAM you have on the machine same as Numpy. However there is a project Blaze which is helping to overcome this limit.\"\n",
    "\n",
    "#### Numpy\n",
    "- https://docs.scipy.org/doc/numpy-dev/user/quickstart.html\n",
    "- https://docs.scipy.org/doc/numpy/reference/routines.html\n",
    "- Array Creation\n",
    "- Printing Arrays\n",
    "- Linear Algebra\n",
    "- Array Product\n",
    "- Matrix Product\n",
    "- Element-wise Universal Functions\n",
    "- Indexing, Slicing, Iterating\n",
    "- Shape Manipulation\n",
    "- Stacking\n",
    "- Splitting / Copying / Views\n",
    "- Shallow / Deep Copy\n",
    "- Basic Statistics\n",
    "- Broadcasting\n",
    "- Histograms\n",
    "\n",
    "#### Pandas\n",
    "- http://pandas.pydata.org/pandas-docs/stable/basics.html\n",
    "- Boolean reductions\n",
    "- missing values\n",
    "- comparison functions\n",
    "- overlapping data sets\n",
    "- descriptive statistics\n",
    "- index of Min/Max Values\n",
    "- discretizing / quantiling\n",
    "- table/row/column/element-wise function application\n",
    "- aggregation\n",
    "- transform\n",
    "- reindex\n",
    "- align\n",
    "- labels\n",
    "- date/time\n",
    "- sorting\n",
    "- searching\n",
    "- multi-index\n",
    "\n",
    "\n",
    "### R Ecosystem - Tidyverse\n",
    "![Tidyverse](00_images/tidyverse_diagram.png)\n",
    "\n",
    "- https://www.tidyverse.org/\n",
    "- http://fg2re.sellorm.com/\n",
    "- https://hawaiimachinelearning.github.io/event/2017/12/18/exploratory-data-analysis-tidyverse/\n",
    "\n",
    "\n",
    "\n",
    "#### dplyr\n",
    "- mutate\n",
    "- select\n",
    "- filter\n",
    "- summarise\n",
    "- arrange\n",
    "\n",
    "#### tidyr\n",
    "- gather\n",
    "- spread\n",
    "\n",
    "#### readr\n",
    "- read_csv\n",
    "- read_tsv\n",
    "- read_delim\n",
    "- read_fwf\n",
    "- read_table\n",
    "- read_log\n",
    "\n",
    "#### purrr\n",
    "- map...\n",
    "\n",
    "#### tibble\n",
    "\n",
    "#### ggplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Ideas\n",
    "\n",
    "- Using a dataset that you care about, use each function in a Pandas to see what it does and how it works\n",
    "- Create a collection of examples of projects on github that use each of the functions you're interested in. \n",
    "- Try to implement functions from the libraries manually and compare the results\n",
    "- Review and modify the tidyverse code from the Machine learning group\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 1/2/3:\n",
      "['tori', 'jon', 'nat', 'runjini', 'michael', 'olina', 'hunter', 'sheuli']\n",
      "Day 4/5/6:\n",
      "['sheuli' 'tori' 'jon' 'nat' 'runjini' 'michael' 'olina' 'hunter']\n"
     ]
    }
   ],
   "source": [
    "#Randomizer\n",
    "import random\n",
    "import numpy\n",
    "cohort = [\"hunter\",\"jon\",\"michael\",\"olina\", \"nat\", \"runjini\", \"sheuli\",\"tori\"]\n",
    "random.shuffle(cohort)\n",
    "\n",
    "print(\"Day 1/2/3:\")\n",
    "print(cohort)\n",
    "cohort = numpy.roll(cohort,1)\n",
    "print(\"Day 4/5/6:\")\n",
    "print(cohort)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
