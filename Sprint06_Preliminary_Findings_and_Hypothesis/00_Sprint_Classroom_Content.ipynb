{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Preliminary Findings and Hypotheses\n",
    "*Module: Exploratory Data Analysis (Sprint 2 of 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sprint Module Review and Data Stories\n",
    "\n",
    "#### Module 3: Exploratory Data Analysis\n",
    "*Data analysis is built around questions, and exploratory data analysis helps you know what questions to ask. Descriptive statistics and basic visualizations that summarize features or suggest relationships inspire the generation of hypotheses to confirm with statistical tests or build into statistical models.*\n",
    "\n",
    "## Sprint 6: Preliminary Findings and Hypotheses\n",
    "|Data Journalist| Data Engineer | Statistical Modeler| Business Analyst |\n",
    "|:----------------:|:----:|:------------------:|:----:|\n",
    "|I need to **identify interesting patterns** so that I can direct further investigation| I need to **understand the volume and data types of data** to understand their performance implications| I need to **produce statistical summaries that explain how variables in my data set relate to each other** so that I can develop hypotheses to guide my analysis| I need to **produce preliminary charts and dashboards** so that I can communicate with other areas of the business about problems we need to solve with joint expertise and refine data collection based on feedback|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Python Chart Library\n",
    "https://python-graph-gallery.com/\n",
    "    \n",
    "http://visualizationuniverse.com/charts/?sortBy=volume&sortDir=desc\n",
    "http://datavizproject.com/\n",
    "    \n",
    "(http://ggplot2.tidyverse.org/). For Python, there is a library which uses the same approach to visualization (http://ggplot.yhathq.com/), but I have also heard great things about Seaborn (http://seaborn.pydata.org). There's also a cool project providing a data interchange format between R and Python (https://github.com/wesm/feather).\n",
    "\n",
    "https://www.statmethods.net/graphs/scatterplot.html\n",
    "    \n",
    "    http://www.sthda.com/english/wiki/ggplot2-scatter-plots-quick-start-guide-r-software-and-data-visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whiteboard Exercise\n",
    "- Illustrate the concepts from sprint 5\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Key Concepts and Definitions\n",
    "#### Sprint 5\n",
    "- sample\n",
    "- statistic\n",
    "- population\n",
    "- parameter\n",
    "- central tendency\n",
    "- variation\n",
    "- univariate\n",
    "- multivariate\n",
    "- distribution\n",
    "- categorical variable\n",
    "- continuous / quantitative variable\n",
    "- variance\n",
    "- standard deviation\n",
    "- interquartile range\n",
    "- skewness\n",
    "- kurtosis\n",
    "- histogram\n",
    "- stem and leaf\n",
    "- box plot\n",
    "- outlier\n",
    "- cross tabulation\n",
    "- anova\n",
    "- correlation\n",
    "- covariance\n",
    "\n",
    "#### Sprint 6\n",
    "- one dimension\n",
    "- multi-dimension\n",
    "- dimensionality\n",
    "- dimensionality reduction\n",
    "- covariation\n",
    "- clustering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R in a Nutshell\n",
    "Data Science from Scratch\n",
    "R For Data Science\n",
    "\n",
    "#### Patterns\n",
    "> Patterns provide one of the most useful tools for data scientists because they reveal covariation. If you think of variation as a phenomenon that creates uncertainty, covariation is a phenomenon that reduces it. If two variables covary, you can use the values of one variable to make better predictions about the values of the second. If the covariation is due to a causal relationship (a special case), then you can use the value of one variable to control the value of the second.\n",
    "\n",
    "#### Correlation and Covariance\n",
    "\n",
    ">Very often, when analyzing data, you want to know if two variables are correlated. Informally, correlation answers the question, “When we increase (or decrease) x, does y increase (or decrease), and by how much?” \n",
    "\n",
    "> Formally, correlation measures the linear dependence between two random variables. Correlation measures range between −1 and 1; 1 means that one variable is a (positive) linear function of the other, 0 means the two variables aren’t correlated at all, and −1 means that one variable is a negative linear function of the other (the two move in completely op- posite directions; \n",
    "\n",
    "#### Correlation Matrix\n",
    ">With many dimensions, you’d like to know how all the dimensions relate to one another. A simple approach is to look at the correlation matrix, in which the entry in row i and column j is the correlation between the ith dimension and the jth dimen‐ sion of the data:\n",
    "\n",
    ">A more visual approach (if you don’t have too many dimensions) is to make a scatter‐ plot matrix (Figure 10-4) showing all the pairwise scatterplots. To do that we’ll use plt.subplots(), which allows us to create subplots of our chart. We give it the num‐ ber of rows and the number of columns, and it returns a figure object (which we won’t use) and a two-dimensional array of axes objects (each of which we’ll plot to):\n",
    "\n",
    "\n",
    "#### Simpson’s Paradox\n",
    "> One not uncommon surprise when analyzing data is Simpson’s Paradox, in which correlations can be misleading when confounding variables are ignored.\n",
    "\n",
    "#### Correlation and Causation\n",
    "> You have probably heard at some point that “correlation is not causation,” most likely by someone looking at data that posed a challenge to parts of his worldview that he was reluctant to question. Nonetheless, this is an important point—if x and y are strongly correlated, that might mean that x causes y, that y causes x, that each causes the other, that some third factor causes both, or it might mean nothing.\n",
    "\n",
    "#### Principal Components Analysis\n",
    "> Another technique for analyzing data is principal components analysis. Principal components analysis breaks a set of (possibly correlated) variables into a set of un- correlated variables.\n",
    "\n",
    "#### Factor Analysis\n",
    "> In most data analysis problems, there are some quantities that we can observe and some that we cannot. The classic examples come from the social sciences. Suppose that you wanted to measure intelligence. It’s not possible to directly measure an abstract concept like intelligence, but it is possible to measure performance on dif- ferent tests. You could use factor analysis to analyze a set of test scores (the observed values) to try to determine intelligence (the hidden value).\n",
    "Factor analysis is available in R through the function factanal in the stats package:\n",
    "\n",
    "#### Bootstrap Resampling\n",
    "> When analyzing statistics, analysts often wonder if the statistics are sensitive to a few outlying values. Would we get a similar result if we were to omit a few points? What is the range of values for the statistic? It is possible to answer these questions for an arbitrary statistic using a technique called bootstrapping.\n",
    "\n",
    "> Formally, bootstrap resampling is a technique for estimating the bias of an estimator. An estimator is a statistic calculated from a data sample that provides an estimate of a true underlying value, often a mean, a standard deviation, or a hidden parameter.\n",
    "\n",
    "> Bootstrapping works by repeatedly selecting random observations from a data sam- ple (with replacement) and recalculating the statistic. In R, you can use bootstrap resampling through the boot function in the boot package:\n",
    "\n",
    "#### Lattice Graphic Package \n",
    "> Lattice functions make it easy to do some things that are hard to do with standard graphics, such as plotting multiple plots on the same page or superimposing plots. Additionally, most lattice functions produce clean, readable output by default. This chapter shows what lattice graphics can do and explains how to use them.\n",
    "\n",
    ">The real strength of the lattice package is in splitting a chart into different panels (shown in a grid), or groups (shown with different colors or symbols) using a con- ditioning or grouping variable. This chapter includes many examples that start with a simple chart and then split it into multiple pieces to answer a question raised by the original plot.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Ideas\n",
    "\n",
    "Explore these datasets\n",
    "https://simplystatistics.org/2018/01/22/the-dslabs-package-provides-datasets-for-teaching-data-science/?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+SimplyStatistics+%28Simply+Statistics%29\n",
    "\n",
    "Find and Replicate a Kaggle Machine Learning EDA workflow\n",
    "https://www.kaggle.com/xchmiao/eda-with-python\n",
    "\n",
    "From Justin for Both Sprints on EDA\n",
    "- What is your intuition about the data? What do you ‘see’ when you look at a time series? What do you ‘see’ when you plot that same series? How does the visualization aid or deny your intuitions? What is the best way to visually encode the data you are currently evaluating?\n",
    "- Histogram everything\n",
    "- Using a csv or excel file of a data series like stock prices, weather data, house prices,  take some basic summary stats\n",
    "- `df.describe` and `df.info` metadata vs summary stats?\n",
    "- Tools: pandas for basic visualization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 1/2/3:\n",
      "['runjini', 'tori', 'nat', 'michael', 'hunter', 'sheuli', 'jon']\n",
      "Day 4/5/6:\n",
      "['jon' 'runjini' 'tori' 'nat' 'michael' 'hunter' 'sheuli']\n"
     ]
    }
   ],
   "source": [
    "#Randomizer\n",
    "import random\n",
    "import numpy\n",
    "cohort = [\"hunter\",\"jon\",\"michael\", \"nat\", \"runjini\", \"sheuli\",\"tori\"]\n",
    "random.shuffle(cohort)\n",
    "\n",
    "print(\"Day 1/2/3:\")\n",
    "print(cohort)\n",
    "cohort = numpy.roll(cohort,1)\n",
    "print(\"Day 4/5/6:\")\n",
    "print(cohort)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Capital</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Belgium</td>\n",
       "      <td>Brussels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>India</td>\n",
       "      <td>New Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>Brasília</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Country    Capital\n",
       "0  Belgium   Brussels\n",
       "1    India  New Delhi\n",
       "2   Brazil   Brasília"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = {'Country': ['Belgium', 'India', 'Brazil'],\n",
    " 'Capital': ['Brussels', 'New Delhi', 'Brasília'],\n",
    " 'Population': [11190846, 1303171035, 207847528]}\n",
    "\n",
    "df = pd.DataFrame(data,\n",
    " columns=['Country', 'Capital', 'Population'])\n",
    "\n",
    "df[['Country', 'Capital']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we know what we CAN'T see in our data?\n",
    "How do we account for biases, false leads?\n",
    "There's a lot of data to go through. So many data sets, how do we get through all of them?\n",
    "How do we address the questions we come up with fast enough and jump from one idea to the next?\n",
    "How much domain knowledge do we need to do Exploratory Data Analysis?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
