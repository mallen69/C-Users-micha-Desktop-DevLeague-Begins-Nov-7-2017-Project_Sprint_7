{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Preliminary Findings and Hypotheses\n",
    "*Module: Exploratory Data Analysis (Sprint 2 of 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sprint Module Review and Data Stories\n",
    "\n",
    "#### Module 3: Exploratory Data Analysis\n",
    "*Data analysis is built around questions, and exploratory data analysis helps you know what questions to ask. Descriptive statistics and basic visualizations that summarize features or suggest relationships inspire the generation of hypotheses to confirm with statistical tests or build into statistical models.*\n",
    "\n",
    "## Sprint 6: Preliminary Findings and Hypotheses\n",
    "|Data Journalist| Data Engineer | Statistical Modeler| Business Analyst |\n",
    "|:----------------:|:----:|:------------------:|:----:|\n",
    "|I need to **identify interesting patterns** so that I can direct further investigation| I need to **understand the volume and data types of data** to understand their performance implications| I need to **produce statistical summaries that explain how variables in my data set relate to each other** so that I can develop hypotheses to guide my analysis| I need to **produce preliminary charts and dashboards** so that I can communicate with other areas of the business about problems we need to solve with joint expertise and refine data collection based on feedback|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Questions from Last EDA\n",
    "- How do we know what we CAN'T see in our data?\n",
    "- How do we account for biases, false leads?\n",
    "- There's a lot of data to go through. So many data sets, how do we get through all of them?\n",
    "- How do we address the questions we come up with fast enough and jump from one idea to the next?\n",
    "- How much domain knowledge do we need to do Exploratory Data Analysis?\n",
    "\n",
    "### Charting ideas\n",
    "- Python Chart Library: https://python-graph-gallery.com/\n",
    "- http://visualizationuniverse.com/charts/?sortBy=volume&sortDir=desc\n",
    "- http://datavizproject.com/\n",
    "- http://ggplot2.tidyverse.org/\n",
    "- http://ggplot.yhathq.com/\n",
    "- http://seaborn.pydata.org\n",
    "- https://github.com/wesm/feather\n",
    "- https://www.statmethods.net/graphs/scatterplot.html\n",
    "- http://www.sthda.com/english/wiki/ggplot2-scatter-plots-quick-start-guide-r-software-and-data-visualization\n",
    "\n",
    "### Histograms, Correlations, and other Statistical Topics\n",
    "- http://flowingdata.com/2017/06/07/how-histograms-work/\n",
    "- http://tinlizzie.org/histograms/\n",
    "- http://rpsychologist.com/d3/correlation/\n",
    "- http://rpsychologist.com/d3/CI/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Sprint Option\n",
    "Objectives\n",
    "- Explore key statistical concepts *(that provide the basis of the preliminary findings)*\n",
    "- Develop rigorous process *(that drive towards preliminary findings)*\n",
    "- Develop Expertise in a dataset that is relevant to you. *(preliminary findings that are relevant to data in your field, hypotheses)*\n",
    "\n",
    "Structure\n",
    "- Week 1: Paired Sprint, develop conceptual depth and process\n",
    "- Week 2: Follow Intuitive Inquiry, Polish, Communicate, and Share findings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Key Concepts and Definitions\n",
    "#### Sprint 5\n",
    "- sample\n",
    "- statistic\n",
    "- population\n",
    "- parameter\n",
    "- central tendency\n",
    "- variation\n",
    "- univariate\n",
    "- multivariate\n",
    "- distribution\n",
    "- categorical variable\n",
    "- continuous / quantitative variable\n",
    "- variance\n",
    "- standard deviation\n",
    "- interquartile range\n",
    "- skewness\n",
    "- kurtosis\n",
    "- histogram\n",
    "- stem and leaf\n",
    "- box plot\n",
    "- outlier\n",
    "- cross tabulation\n",
    "- anova\n",
    "- correlation\n",
    "- covariance\n",
    "\n",
    "#### Sprint 6\n",
    "- one dimension\n",
    "- multi-dimension\n",
    "- scatterplot\n",
    "- scatterplot matrix\n",
    "- dimensionality\n",
    "- dimensionality reduction\n",
    "- covariation\n",
    "- clustering\n",
    "- k-means\n",
    "- feature extraction\n",
    "- feature selection / elimination\n",
    "- feature engineering\n",
    "- principle component analysis\n",
    "- factor analysis\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R in a Nutshell\n",
    "Data Science from Scratch\n",
    "R For Data Science\n",
    "\n",
    "#### Patterns\n",
    "> Patterns provide one of the most useful tools for data scientists because they reveal covariation. If you think of variation as a phenomenon that creates uncertainty, covariation is a phenomenon that reduces it. If two variables covary, you can use the values of one variable to make better predictions about the values of the second. If the covariation is due to a causal relationship (a special case), then you can use the value of one variable to control the value of the second.\n",
    "\n",
    "#### Correlation and Covariance\n",
    "\n",
    ">Very often, when analyzing data, you want to know if two variables are correlated. Informally, correlation answers the question, “When we increase (or decrease) x, does y increase (or decrease), and by how much?” \n",
    "\n",
    "> Formally, correlation measures the linear dependence between two random variables. Correlation measures range between −1 and 1; 1 means that one variable is a (positive) linear function of the other, 0 means the two variables aren’t correlated at all, and −1 means that one variable is a negative linear function of the other (the two move in completely op- posite directions; \n",
    "\n",
    "#### Correlation Matrix\n",
    ">With many dimensions, you’d like to know how all the dimensions relate to one another. A simple approach is to look at the correlation matrix, in which the entry in row i and column j is the correlation between the ith dimension and the jth dimen‐ sion of the data:\n",
    "\n",
    ">A more visual approach (if you don’t have too many dimensions) is to make a scatter‐ plot matrix (Figure 10-4) showing all the pairwise scatterplots. To do that we’ll use plt.subplots(), which allows us to create subplots of our chart. We give it the num‐ ber of rows and the number of columns, and it returns a figure object (which we won’t use) and a two-dimensional array of axes objects (each of which we’ll plot to):\n",
    "\n",
    "\n",
    "#### Simpson’s Paradox\n",
    "> One not uncommon surprise when analyzing data is Simpson’s Paradox, in which correlations can be misleading when confounding variables are ignored.\n",
    "\n",
    "#### Correlation and Causation\n",
    "> You have probably heard at some point that “correlation is not causation,” most likely by someone looking at data that posed a challenge to parts of his worldview that he was reluctant to question. Nonetheless, this is an important point—if x and y are strongly correlated, that might mean that x causes y, that y causes x, that each causes the other, that some third factor causes both, or it might mean nothing.\n",
    "\n",
    "#### Dimensionality Reduction\n",
    "> You might ask the question, “How do I take all of the variables I’ve collected and focus on only a few of them?” In technical terms, you want to “reduce the dimension of your feature space.” By reducing the dimension of your feature space, you have fewer relationships between variables to consider and you are less likely to overfit your model. (Note: This doesn’t immediately mean that overfitting, etc. are no longer concerns — but we’re moving in the right direction!)\n",
    "\n",
    ">Somewhat unsurprisingly, reducing the dimension of the feature space is called “dimensionality reduction.” There are many ways to achieve dimensionality reduction, but most of these techniques fall into one of two classes:\n",
    "\n",
    ">- Feature Elimination\n",
    ">- Feature Extraction\n",
    "\n",
    "#### Principal Components Analysis\n",
    "> Another technique for analyzing data is principal components analysis. Principal components analysis breaks a set of (possibly correlated) variables into a set of un- correlated variables.\n",
    "\n",
    "> A brief description\n",
    "> https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c\n",
    "\n",
    "#### Factor Analysis\n",
    "> In most data analysis problems, there are some quantities that we can observe and some that we cannot. The classic examples come from the social sciences. Suppose that you wanted to measure intelligence. It’s not possible to directly measure an abstract concept like intelligence, but it is possible to measure performance on dif- ferent tests. You could use factor analysis to analyze a set of test scores (the observed values) to try to determine intelligence (the hidden value).\n",
    "Factor analysis is available in R through the function factanal in the stats package:\n",
    "\n",
    "#### Bootstrap Resampling\n",
    "> When analyzing statistics, analysts often wonder if the statistics are sensitive to a few outlying values. Would we get a similar result if we were to omit a few points? What is the range of values for the statistic? It is possible to answer these questions for an arbitrary statistic using a technique called bootstrapping.\n",
    "\n",
    "> Formally, bootstrap resampling is a technique for estimating the bias of an estimator. An estimator is a statistic calculated from a data sample that provides an estimate of a true underlying value, often a mean, a standard deviation, or a hidden parameter.\n",
    "\n",
    "> Bootstrapping works by repeatedly selecting random observations from a data sam- ple (with replacement) and recalculating the statistic. In R, you can use bootstrap resampling through the boot function in the boot package:\n",
    "\n",
    "#### Lattice Graphic Package \n",
    "> Lattice functions make it easy to do some things that are hard to do with standard graphics, such as plotting multiple plots on the same page or superimposing plots. Additionally, most lattice functions produce clean, readable output by default. This chapter shows what lattice graphics can do and explains how to use them.\n",
    "\n",
    ">The real strength of the lattice package is in splitting a chart into different panels (shown in a grid), or groups (shown with different colors or symbols) using a con- ditioning or grouping variable. This chapter includes many examples that start with a simple chart and then split it into multiple pieces to answer a question raised by the original plot.\n",
    "\n",
    "#### k-means\n",
    "\n",
    "> Your goal is to segment the users. This process is known by various names: besides being called segmenting, you could say that you’re go‐ ing to stratify, group, or cluster the data. They all mean finding similar types of users and bunching them together.\n",
    "Why would you want to do this? Here are a few examples:\n",
    "\n",
    "> • You might want to give different users different experiences. Mar‐ keting often does this; for example, to offer toner to people who are known to own printers.\n",
    "\n",
    "> • You might have a model that works better for specific groups. Or you might have different models for different groups.\n",
    "\n",
    "> • Hierarchical modeling in statistics does something like this; for example, to separately model geographical effects from household effects in survey results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Ideas\n",
    "\n",
    "Explore these datasets\n",
    "https://simplystatistics.org/2018/01/22/the-dslabs-package-provides-datasets-for-teaching-data-science/?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+SimplyStatistics+%28Simply+Statistics%29\n",
    "\n",
    "Find and Replicate a Kaggle Machine Learning EDA workflow\n",
    "https://www.kaggle.com/xchmiao/eda-with-python\n",
    "\n",
    "From Justin for Both Sprints on EDA\n",
    "- What is your intuition about the data? What do you ‘see’ when you look at a time series? What do you ‘see’ when you plot that same series? How does the visualization aid or deny your intuitions? What is the best way to visually encode the data you are currently evaluating?\n",
    "- Histogram everything\n",
    "- Using a csv or excel file of a data series like stock prices, weather data, house prices,  take some basic summary stats\n",
    "- `df.describe` and `df.info` metadata vs summary stats?\n",
    "- Tools: pandas for basic visualization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 1/2/3:\n",
      "['runjini', 'tori', 'nat', 'michael', 'hunter', 'sheuli', 'jon']\n",
      "Day 4/5/6:\n",
      "['jon' 'runjini' 'tori' 'nat' 'michael' 'hunter' 'sheuli']\n"
     ]
    }
   ],
   "source": [
    "#Randomizer\n",
    "import random\n",
    "import numpy\n",
    "cohort = [\"hunter\",\"jon\",\"michael\", \"runjini\", \"sheuli\",\"tori\"]\n",
    "random.shuffle(cohort)\n",
    "\n",
    "print(\"Day 1/2/3:\")\n",
    "print(cohort)\n",
    "cohort = numpy.roll(cohort,1)\n",
    "print(\"Day 4/5/6:\")\n",
    "print(cohort)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
